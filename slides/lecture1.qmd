---
title: "Day 1"
subtitle: "NORSAR ML Workshop"
author: "steffen.maeland@norsar.no"
format:
  revealjs:
    auto-stretch: false
    auto-play-media: true
---

## Agenda

:::{.columns}
:::{.column width="50%"}

### Today

- Pizza

:::
:::{.column width="50%"}

### Tomorrow

:::
:::

## What, why, wow {.center background-color="#F3E5F5"}

## The _what_

:::{.columns}
:::{.column width="60%"}
- AI: Computer makes smart decisions
- Machine learning: Computer learns to recognise patterns
- Deep learning: Computer learns to recognise advanced patterns
- (Generative DL)

The big AI tools of today are driven by advancements in

- Deep learning -- bigger and better models
- Computing -- bigger and better data centres
:::
:::

:::{.absolute top="10%" right="0%" width="400px"}
![](figures/AI_hierarchy.png)
:::

## The _what_

:::{.columns}
:::{.column width="50%"}
Traditional approach: (_symbolic AI_)

IF amplitude > threshold AND duration > 2 seconds
THEN earthquake;
ELSE noise;
:::

:::{.column width="50%"}
These are earthquakes, these are not, figure out the difference

(figures)
:::
:::

## The _what_

Some terminology:

- _Model:_
- _Label:_



## The _why_


## The _wow_

Success stories


## Exercise number 1 {.center background-color="#FBE9E7"}

## Post-exercise: understanding what we did

## ML for waveform data

Cross-correlation

Convolutions

## Neural networks

Sequentially improved features

## {background-iframe="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=15&networkShape=&seed=0.56524&showTestData=false&discretize=false&percTrainData=50&x=false&y=false&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&playButton_hide=false&showTestData_hide=false&regularization_hide=true&dataset_hide=false&batchSize_hide=true&learningRate_hide=false&percTrainData_hide=true&regularizationRate_hide=true&problem_hide=true" background-interactive="true" data-menu-title="TensorFlow playground"}

:::{.absolute bottom="0%" left="0%" style="font-size: 0.5em;"}
[_TensorFlow<br>playground_](https://playground.tensorflow.org/)
:::

## _Deep_ learning {background-color="#EDE7F6"}

:::{style="padding-top: 80px;"}
:::

The point of deep learning is to sequentially **learn better feature representations**, and use these to solve a task.

:::{style="padding-top: 80px;"}
:::

Since neural networks are _universal function approximators_, they can model arbitrarily complex relationships. The cost of doing so, is that we need a lot of data.


## Enter the _convolution operation_

The foundation for modern computer vision [_(plus lots of other things!)_]{.color-grey}<br>
is [**convolution**]{.color-purple}:

an operation that takes in two functions and returns a new function

:::{style="padding-top: 40px;"}
:::

::::{.columns}
:::{.column width="50%}
$$
f \ast g \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$
:::

:::{.column width="50%}
:::{.fragment}
![](https://upload.wikimedia.org/wikipedia/commons/6/6a/Convolution_of_box_signal_with_itself2.gif)
:::
:::

::::



## Enter the _convolution operation_

The foundation for modern computer vision [_(plus lots of other things!)_]{.color-grey}<br>
is [**convolution**]{.color-purple}:

an operation that takes in two functions and returns a new function

:::{style="padding-top: 40px;"}
:::

::::{.columns}
:::{.column width="50%}
$$
f \ast g \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$
:::

:::{.column width="50%}
![](https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif)
:::

::::

In practice, convolution is a way to **recognise and localise patterns** in data


## Discrete convolution

Convolution is a lot easier with discrete data such as images, because:

 - the integral becomes a [sum]{.color-pink-dark}
 - the first function is our [image]{.color-pink-dark}
 - the second function is our [_**kernel**_]{.color-pink-dark} or [_**filter**_]{.color-pink-dark}, which tries to find patterns in the image.



## Convolutions recap

![](figures/conv1d-0.png)

## Convolutions recap

![](figures/conv1d-1.png)

## Convolutions recap

![](figures/conv1d-2.png)

## Convolutions recap

![](figures/conv1d-3.png)

## Convolutions recap

![](figures/conv1d-4.png)

## Convolutions recap

![](figures/conv1d-last.png)

## Convolutions detour

If the take the convolution operation

$$
\small
(f \ast g)(t) \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$

but reverse one of the functions ($\small f(t) \to f(-t)$), we get the similar operation called _cross-correlation_:

:::{style="margin-top: -30px;"}
$$
\small
f \star g \equiv f(-t) \ast g(t)
$$
:::

:::{.fragment}
![](https://upload.wikimedia.org/wikipedia/commons/7/71/Cross_correlation_animation.gif)
:::

## Convolutions detour

If the take the convolution operation

$$
\small
f \ast g \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$

but reverse one of the functions ($\small f(t) \to f(-t)$), we get the similar operation called _cross-correlation_:

:::{style="margin-top: -30px;"}
$$
\small
f \star g \equiv f(-t) \ast g(t)
$$
:::

![](figures/box-crosscorrelation.png){fig-align="center" width="550px" style="margin-top: 0px;"}


:::{.fragment .fade-out .rectangle .absolute bottom="-20px" left="200px" width="650px" height="100px" style="background-color: #fff; border: 0px;"}
:::

## Break {.center background-color="#23e323"}


## Seismology tasks solved with ML / current SOTA


## Exercise 2


## Post-exercise









## Augmentation



Move to day 2

## The more advanced stuff

Deep learning components

More tomorrow!
