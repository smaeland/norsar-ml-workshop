---
title: "Day 1"
subtitle: "NORSAR ML Workshop"
author: "steffen.maeland@norsar.no"
format:
  revealjs:
    auto-stretch: false
    auto-play-media: true
---

## Agenda

:::{.columns}
:::{.column width="50%"}

### Today

|   |   |
|---|---|---|
|  9.00 | Intro |
|  9.30 | Exercise: Event detection |
| 10.00 | Post-exercise: Understanding what we did |
| 11.00 | Exercise: Event classification |
| 11.45 | ✨Special✨ lunch |
| 12.30 | Discussion |
| 13.00 | End of day 1 |

:::
:::{.column width="50%"}

### Tomorrow

|   |   |
|---|---|
|  9.00 | Recap from yesterday |
|  9.15 | Deep learning tools |
|  9.30 | Exercise: Training deep learning models |
| 10.00 | Post-exercise: Finding the optimal model |
| 10.30 | Recent and future ML at NORSAR |
| 11.00 | Hackathon |
| 11.45 | Lunch (kantinen) |
| 12.30 | Wrap-up and discussions |
| 13.00 | End of day 2 |
:::
:::


## What, why, wow {.center background-color="#F3E5F5"}

## The _what_

:::{.columns}
:::{.column width="60%"}
- AI: Computer makes smart decisions
- Machine learning: Computer learns to recognise patterns
- Deep learning: Computer learns to recognise advanced patterns
- (Generative DL)

The big AI tools of today are driven by advancements in

- Deep learning -- bigger and better models
- Computing -- bigger and better data centres
:::
:::

:::{.absolute top="10%" right="0%" width="400px"}
![](figures/AI_hierarchy.png)
:::

## The _what_

:::{.columns}
:::{.column width="50%"}
Traditional approach: (_symbolic AI_)

IF amplitude > threshold AND duration > 2 seconds
THEN earthquake;
ELSE noise;
:::

:::{.column width="50%"}
Machine learning approach:

This is an earthquake

![](https://raw.githubusercontent.com/smousavi05/STEAD/master/eventSample.png)

This is not

![](https://raw.githubusercontent.com/smousavi05/STEAD/master/noise.png)

Compute a function to separate the two.

:::
:::

## The _what_

Let's plot number of results on [Google Scholar](https://scholar.google.com)

:::{.absolute left="5%" bottom="5%" width="800px"}
![](figures/papers_per_year.png)
:::


## The _what_

Some terminology:

- _Model:_
- _Label:_
- _Supervised learning:_


And some statistics:

(polynomial regression animation?)


## The _what_

Datasets, data quality



## The _what_

Today and tomorrow:

Intro to modern ML technologies, which is mainly **pattern recognition**.

:::{.absolute bottom="0%" left="0%" width="200px"}
![](https://m.media-amazon.com/images/I/71fqxXDY2ZL._UF1000,1000_QL80_.jpg)
:::

:::{.absolute bottom="0%" right="0%" width="200px"}
![](https://www.bishopbook.com/assets/images/deep-learning-book-cover.jpg)
:::

## The _why_

Accessible tech!


## The _wow_

Success stories


## Exercise number 1 {.center background-color="#FBE9E7"}

## Post-exercise {.center background-color="#E0F2F1"}

Understanding what we did


## Fun with crosscorrelation

Animation


## ML for waveform data

Cross-correlation

Convolutions

## Neural networks

Sequentially improved features

## {background-iframe="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=15&networkShape=&seed=0.56524&showTestData=false&discretize=false&percTrainData=50&x=false&y=false&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&playButton_hide=false&showTestData_hide=false&regularization_hide=true&dataset_hide=false&batchSize_hide=true&learningRate_hide=false&percTrainData_hide=true&regularizationRate_hide=true&problem_hide=true" background-interactive="true" data-menu-title="TensorFlow playground"}

:::{.absolute bottom="0%" left="0%" style="font-size: 0.5em;"}
[_TensorFlow<br>playground_](https://playground.tensorflow.org/)
:::

## _Deep_ learning {background-color="#EDE7F6"}

:::{style="padding-top: 80px;"}
:::

The point of deep learning is to sequentially **learn better feature representations**, and use these to solve a task.

:::{style="padding-top: 80px;"}
:::

Since neural networks are _universal function approximators_, they can model arbitrarily complex relationships. The cost of doing so, is that we need a lot of data.


## Enter the _convolution operation_

The foundation for modern computer vision [_(plus lots of other things!)_]{.color-grey}<br>
is [**convolution**]{.color-purple}:

an operation that takes in two functions and returns a new function

:::{style="padding-top: 40px;"}
:::

::::{.columns}
:::{.column width="50%}
$$
f \ast g \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$
:::

:::{.column width="50%}
:::{.fragment}
![](https://upload.wikimedia.org/wikipedia/commons/6/6a/Convolution_of_box_signal_with_itself2.gif)
:::
:::

::::



## Enter the _convolution operation_

The foundation for modern computer vision [_(plus lots of other things!)_]{.color-grey}<br>
is [**convolution**]{.color-purple}:

an operation that takes in two functions and returns a new function

:::{style="padding-top: 40px;"}
:::

::::{.columns}
:::{.column width="50%}
$$
f \ast g \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$
:::

:::{.column width="50%}
![](https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif)
:::

::::

In practice, convolution is a way to **recognise and localise patterns** in data


## Discrete convolution

Convolution is a lot easier with discrete data such as images, because:

 - the integral becomes a [sum]{.color-pink-dark}
 - the first function is our [image]{.color-pink-dark}
 - the second function is our [_**kernel**_]{.color-pink-dark} or [_**filter**_]{.color-pink-dark}, which tries to find patterns in the image.



## Convolutions recap

![](figures/conv1d-0.png)

## Convolutions recap

![](figures/conv1d-1.png)

## Convolutions recap

![](figures/conv1d-2.png)

## Convolutions recap

![](figures/conv1d-3.png)

## Convolutions recap

![](figures/conv1d-4.png)

## Convolutions recap

![](figures/conv1d-last.png)

## Convolutions detour

If the take the convolution operation

$$
\small
(f \ast g)(t) \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$

but reverse one of the functions ($\small f(t) \to f(-t)$), we get the similar operation called _cross-correlation_:

:::{style="margin-top: -30px;"}
$$
\small
f \star g \equiv f(-t) \ast g(t)
$$
:::

:::{.fragment}
![](https://upload.wikimedia.org/wikipedia/commons/7/71/Cross_correlation_animation.gif)
:::

## Convolutions detour

If the take the convolution operation

$$
\small
f \ast g \equiv \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau
$$

but reverse one of the functions ($\small f(t) \to f(-t)$), we get the similar operation called _cross-correlation_:

:::{style="margin-top: -30px;"}
$$
\small
f \star g \equiv f(-t) \ast g(t)
$$
:::

![](figures/box-crosscorrelation.png){fig-align="center" width="550px" style="margin-top: 0px;"}


:::{.fragment .fade-out .rectangle .absolute bottom="-20px" left="200px" width="650px" height="100px" style="background-color: #fff; border: 0px;"}
:::

## Break {.center background-color="#23e323"}


## Seismology tasks solved with ML / current SOTA


## Exercise 2

Train an ML pick detector


## Post-exercise









## Augmentation



Move to day 2



<!---------------------------------------------------------------------------->
<!---------------------------------------------------------------------------->
## DAY 2


## Recap from day 1



## The more advanced stuff

Deep learning components

## Libraries for machine learning

## Exercise: Unsupervised learning with pretrained image models?




## SOTA research


## NORSAR developments


## Future directions at NORSAR


## Hackathon
