{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3729b16f-d2b5-4ec2-9dd5-7250b82a1ce1",
   "metadata": {},
   "source": [
    "# Use pretrained models for clustering\n",
    "\n",
    "Tired of training models? Just download a pretrained one!\n",
    "\n",
    "Pretrained models for different seismology tasks can be found for instance [here](https://github.com/seisbench/seisbench), but let's do something different.\n",
    "\n",
    "An interesting (and recently popular) method of clustering similar waveforms is to convert them into an image, and then use computer vision models, which are highly advanced and are easily downloaded with pretrained weights. And converting waveforms to images is something we know how to do -- we can compute spectrograms.\n",
    "\n",
    "Now computer vision models are typically trained to classify different objects, and not look at spectrograms. We'll do a little trick to use their pattern recognition abilities to cluster images instead. But first, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7d9fe-71ef-4992-8c44-0fdce01c3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import h5py\n",
    "import obspy\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c57c-d37e-4071-ba80-354c19b5028b",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a17a29-b78a-4c1b-a042-c4177fd094ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/norsar-ml-ws/events_classification_Zonly_TRAIN.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63481e-4e82-416c-ab75-715e21d387c2",
   "metadata": {},
   "source": [
    "## Create spectrograms\n",
    "\n",
    "Here we can again make use of the nice Obspy library. For visualisation, we can plot the spectrogram for an event along with the original waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46d1e8-20ee-4206-9a4d-e04f2e68322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('events_classification_Zonly_TRAIN.h5', \"r\") as fin:\n",
    "    waveforms = fin.get('waveforms')[:100]\n",
    "    event_types = fin.get('type')[:]\n",
    "\n",
    "event_index = 1\n",
    "sampling_freq = 100\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "time_axis = np.arange(len(waveforms[event_index, :, 0])) / sampling_freq  # Convert samples to time\n",
    "plt.plot(time_axis, waveforms[event_index, :, 0], 'b-', linewidth=0.8)\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf840f8-039d-4c72-8e02-f5c4c9641c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.imaging.spectrogram import spectrogram \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), \n",
    "                               gridspec_kw={'height_ratios': [2, 1]}, \n",
    "                               sharex=True)\n",
    "\n",
    "# Spectrogram\n",
    "ax1.set_ylabel('Frequency (Hz)')\n",
    "spectrogram(waveforms[event_index, :, 0], sampling_freq, axes=ax1, show=False)\n",
    "\n",
    "# Waveform \n",
    "time_axis = np.arange(len(waveforms[event_index, :, 0])) / sampling_freq  # Convert samples to time\n",
    "ax2.plot(time_axis, waveforms[event_index, :, 0], 'b-', linewidth=0.8)\n",
    "ax2.set_xlabel('Time (seconds)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feb295-7e3b-42ea-93da-d466e0a9919a",
   "metadata": {},
   "source": [
    "## Write spectrograms to PNG files\n",
    "\n",
    "The computer vision models we'll use come with some nice convenience functions for reading in image files, so let's first write our spectrograms to PNG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b64ef-deae-4f25-a153-24c8f92933ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spectrogram(waveform, fs, filename):\n",
    "\n",
    "    fig = spectrogram(waveform, fs, show=False)\n",
    "    for ax in fig.get_axes():\n",
    "        ax.set_ylim(0.1, 25)\n",
    "        ax.set_xlim(1, 59)\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    fig.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72565d-cab6-4fee-bc2d-46fc337fda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path('spectrograms')\n",
    "image_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for index in range(len(waveforms)):\n",
    "\n",
    "    if index % 10 == 0:\n",
    "        print('Computing spectrogram for event', index)\n",
    "\n",
    "    write_spectrogram(\n",
    "        waveforms[index, :, 0],\n",
    "        sampling_freq,\n",
    "        Path(image_dir / f'event_{index}.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742619e7-f92c-4ee3-a6a7-4e894f21b4a6",
   "metadata": {},
   "source": [
    "Which files do we now have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89e27e-6623-4b7c-ba03-13390e89c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(image_dir.iterdir())\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113cb17-f606-473b-9f0d-534032f8d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that we can read back the image. \n",
    "\n",
    "img = keras.utils.load_img(files[0], target_size=(224, 224))\n",
    "x = keras.utils.img_to_array(img)\n",
    "x /= 255.\n",
    "#x = np.expand_dims(x, axis=0)\n",
    "\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3298cb8-f03a-412d-828e-482b18531a0e",
   "metadata": {},
   "source": [
    "## Download a pretrained computer vision model\n",
    "\n",
    "With Keras it's easy to download pretrained models, we just import the one we want, and then call it. We can start with one named **ResNet50**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8833960-d9b1-4912-8874-e7f45c4da84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793db899-9ce2-42df-9d63-81817a1c3b8c",
   "metadata": {},
   "source": [
    "Now here comes the trick: Instead of using the full model, which is trained to do classification, we run it _without_ the last classification layer. Then we just get out the final features detected in the images, but without any classification step applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96eab6-b866-4c00-bab5-d738db070c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afac529-63c6-4c25-a60d-e32c3c82afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all the images to extract features\n",
    "\n",
    "images = []\n",
    "preds = []\n",
    "for fin in files:\n",
    "    img = keras.utils.load_img(fin, target_size=(224, 224))\n",
    "    x = keras.utils.img_to_array(img)\n",
    "\n",
    "    images.append(x / 255.)\n",
    "    \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds.append(\n",
    "        model.predict(x, verbose=0)\n",
    "    )\n",
    "\n",
    "images = np.stack(images, axis=0)\n",
    "preds = np.vstack(preds)\n",
    "\n",
    "print('images.shape:', images.shape)\n",
    "print('preds.shape:', preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad8fc6-886f-4bc6-b5aa-c2154852038c",
   "metadata": {},
   "source": [
    "## Make use of the output\n",
    "\n",
    "Okay! From the `shapes` we printed above, we see that we for the 100 images, get a list of 2048 numbers. A 2048-demensional space is hard to visualise, but we can use a dimensionality reduction technique to plot it in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578798f4-af3b-4780-873b-9aa853e39d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e5edb-186e-49ad-a307-05fc729baf42",
   "metadata": {},
   "source": [
    "(If not running in Colab, this need `libopenblas-dev`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474a68d-e5d0-496e-b7fd-c27cde83751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = TSNE()\n",
    "\n",
    "embedding = reducer.fit_transform(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d2eb6-b3ea-4585-8841-48158a67ce99",
   "metadata": {},
   "source": [
    "Let's plot the so-called embeddings of our events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa9978-2c94-4bad-827d-57552c5e2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23636ab9-7767-421e-8e70-edcf710f2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def plot_tsne_with_thumbnails(tsne_coords, images, indices_to_show=None, \n",
    "                              thumbnail_size=0.2, figsize=(10, 7)):\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot all points as scatter plot\n",
    "    ax.scatter(tsne_coords[:, 0], tsne_coords[:, 1], s=30)\n",
    "    \n",
    "    for idx in indices_to_show:\n",
    "        img = images[idx]\n",
    "        x, y = tsne_coords[idx]\n",
    "\n",
    "        imagebox = OffsetImage(img, zoom=thumbnail_size)\n",
    "        \n",
    "        ab = AnnotationBbox(imagebox, (x, y), frameon=False, pad=0)\n",
    "        ax.add_artist(ab)\n",
    "    \n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Select random indices to show thumbnails\n",
    "n_thumbnails = 10\n",
    "indices_to_show = np.random.choice(len(images), n_thumbnails, replace=False)\n",
    "\n",
    "fig, ax = plot_tsne_with_thumbnails(\n",
    "    embedding, \n",
    "    images, \n",
    "    indices_to_show=indices_to_show,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
