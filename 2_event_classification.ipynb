{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4f86c8-1b3e-4d4a-8890-65ac461fbd81",
   "metadata": {},
   "source": [
    "# Deep learning-based event classification\n",
    "\n",
    "For this next exercise, it's time to start building deep learning models ourselves.\n",
    "\n",
    "The scenario is to read in waveforms of unknown content, and have a neural network evaluate they contain signal from an earthquake event, or just noise. To achieve this we need\n",
    "\n",
    "1. to build a suitable neural networks, and\n",
    "2. to train it.\n",
    "\n",
    "For point 2, we need labelled data, and ideally as much of it as possible. We start out by downloading files with our training and testing data, containing 100k and 10k waveforms, respectively. In total it's around 2.5GB, so it might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1216b4a-a66c-4c00-8331-a69bb44f7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --no-check-certificate \"https://drive.usercontent.google.com/download?id=1jD0RetG2ZGZdFaZKF7O0y6Ogzzj9fgFz&confirm=t\" -O \"events_classification_Zonly_TRAIN.h5\"\n",
    "! wget --no-check-certificate \"https://drive.usercontent.google.com/download?id=1jQ7Cf0W1dM5VgmOCB6gu08ddHdYmuh3F&confirm=t\" -O \"events_classification_Zonly_TEST.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79061d-6ffe-4d48-a17f-802b483a7789",
   "metadata": {},
   "source": [
    "Import the usual libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abee64-d5f1-4b52-af25-29ba498ecd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf4ce7-0cd1-4862-a743-f539ba742c14",
   "metadata": {},
   "source": [
    "## Load data from files\n",
    "\n",
    "With the files in place, we can load the contents.\n",
    "\n",
    "In case you are running this on a laptop, remenber that we need around 3GB to load it all in memory (so you might want to close some Chrome tabs first). \n",
    "\n",
    "First, the training data.\n",
    "\n",
    "**To be nice to the memory of the machine, we can set the maximum number of events to load.** If set at `None` we load all events in the file, so if you run into trouble, consider setting it to e.g. 10000 and see how that goes. If everthing runs fine, you can raise it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f62987-ca0a-4578-aa51-63222afe1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you change this, also re-run the following two code cells.\n",
    "max_events = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46935c60-a4b6-4883-b5a5-e574d8b503f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('events_classification_Zonly_TRAIN.h5') as train_file:\n",
    "\n",
    "    wf_dataset = train_file.get('waveforms')\n",
    "    label_dataset = train_file.get('type') \n",
    "\n",
    "    if max_events is None:\n",
    "        train_waveforms = wf_dataset[:]\n",
    "        train_labels = label_dataset[:]\n",
    "    else:\n",
    "        train_waveforms = wf_dataset[:max_events]\n",
    "        train_labels = label_dataset[:max_events]\n",
    "\n",
    "# We need to adjust the shape of this one, for technical reasons\n",
    "train_labels = np.expand_dims(train_labels, axis=-1)\n",
    "\n",
    "# Normalise the waveforms\n",
    "max_vals = np.max(np.abs(train_waveforms), axis=1, keepdims=True)\n",
    "train_waveforms /= (max_vals + 1e-8)\n",
    "\n",
    "# Check the shapes:\n",
    "print('train_waveforms.shape:', train_waveforms.shape)\n",
    "print('train_labels.shape:', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccc1ae-9c93-4577-bdbe-f0f083fe1bec",
   "metadata": {},
   "source": [
    "Then the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8e87d-0793-44d0-ba6a-4fd367ab56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('events_classification_Zonly_TEST.h5') as test_file:\n",
    "\n",
    "    wf_dataset = test_file.get('waveforms')\n",
    "    label_dataset = test_file.get('type') \n",
    "\n",
    "    if max_events is None:\n",
    "        test_waveforms = wf_dataset[:]\n",
    "        test_labels = label_dataset[:]\n",
    "    else:\n",
    "        test_waveforms = wf_dataset[:max_events]\n",
    "        test_labels = label_dataset[:max_events]\n",
    "\n",
    "test_labels = np.expand_dims(test_labels, axis=-1)\n",
    "\n",
    "# Normalise \n",
    "max_vals = np.max(np.abs(test_waveforms), axis=1, keepdims=True)\n",
    "test_waveforms /= (max_vals + 1e-8)\n",
    "\n",
    "# Check the shapes:\n",
    "print('test_waveforms.shape:', test_waveforms.shape)\n",
    "print('test_labels.shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc84275-92fe-4449-875c-e953395c57bf",
   "metadata": {},
   "source": [
    "## Build a classification model\n",
    "\n",
    "Let's start assembling the neural network. Our first model will have inputs sequentially processed through a series of layers, so we can specify it as an ordered list of layers, and give it to `keras.Sequential`.\n",
    "\n",
    "Because we want to detect patterns at any location in the waveforms, we choose convolutional layers (`Conv1D`) as the backbone of our network. To look at increasingly longer sequences of the input, we also downsample after each convolution, using `MaxPooling1D` layers.\n",
    "\n",
    "Finally, we collect the detected patterns using a `GlobalMaxPooling` layer, and add a `Dense` layer with a single output at the end, which will be our class prediction. Note that we use the _sigmoid_ activation function, which ensures that the prediction will be between 0 (noise) and 1 (signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3db6e9-94d1-49f5-8978-4d2abd86790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(train_waveforms.shape[1], train_waveforms.shape[2])),\n",
    "        \n",
    "        keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
    "        keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
    "        keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
    "        \n",
    "        keras.layers.GlobalMaxPooling1D(),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22964a-dd19-4a1b-ac87-cec1a06e44ab",
   "metadata": {},
   "source": [
    "To print the structure of our network, we kan use `summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169fa40-e659-4566-9af9-647e36582658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2bcb0-f026-4d62-a75a-5d86d8751c4c",
   "metadata": {},
   "source": [
    "Cool. In order to train the model, we need a success criterion, also known as the _loss function_. This will be high if we are making bad predictions, and zero if we are making perfect predictions. \n",
    "\n",
    "The `compile()` function can take a lot of arguments, but now we just specify the loss function typically used for binary classification, and add that we want to compute the accuracy of predictions during training, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221f235-b899-45fe-b1de-dd6f3eaba457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18030ac3-5a0a-4cb2-a801-70671ff3a8cc",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now for the computationally intensive bit: Minimising the loss funtion. \n",
    "\n",
    "Two important options here:\n",
    "\n",
    "- **epochs:** The number of times to iterate over the entire dataset\n",
    "- **batch_size:** How many events to group together when running gradient descent\n",
    "\n",
    "Further options are described in the [documentation](https://keras.io/api/models/model_training_apis/#fit-method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bfc23-fd5b-41eb-90e3-d1c8d0c354ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_waveforms,\n",
    "    train_labels,\n",
    "    validation_data=(test_waveforms, test_labels),\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c8548-8528-478a-94be-d7d8c07848ef",
   "metadata": {},
   "source": [
    "During training, we ideally want to see the loss value going down, and converge to a low value. Let's investigate a little first, and then tune our settings later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed71ca7-defb-4792-97df-e42eac10174a",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "With our model trained and ready to go, it's time to evaluate performance!\n",
    "\n",
    "First we can compute the accuracy on the independent training data (accuracy of 0 means all predictions are wrong, 0.5 equals random guessing, and 1 means all predictions are correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64c85b-7e19-4610-8c9b-f7faa78915ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_waveforms, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f41e7-2c8f-4377-81c6-d3ba7e87c3bc",
   "metadata": {},
   "source": [
    "We can also plot a few events and visually inspect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89089f9b-d64d-4915-8024-92a260de3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(event_index):\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Select from the test data.\n",
    "    waveform = test_waveforms[event_index, :, :]\n",
    "    waveform = np.expand_dims(waveform, axis=0)\n",
    "\n",
    "    true_label = test_labels[event_index]\n",
    "    \n",
    "    prediction = model(waveform)[0]\n",
    "    \n",
    "    true_type = 'Noise' if true_label < 0.5 else 'Earthquake'\n",
    "    pred_type = 'Noise' if prediction < 0.5 else 'Earthquake'\n",
    "    \n",
    "    plt.plot(np.arange(waveform.shape[1]), waveform[0, :, 0])\n",
    "\n",
    "    plt.title(f'True label: {true_type}, predicted: {pred_type} (confidence: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c7739-02d2-4cde-80e7-372942fd266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(event_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b45a7-de58-47d6-bedc-23562105cf84",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now it's your turn to improve on the model! Things to try:\n",
    "\n",
    "- Increasing the epoch number\n",
    "- Adding more `Conv1D` layers to the model\n",
    "- Tuning the options of the `Conv1D` layer: Increasing filters, kernel sizes, stride lengths... See options [here](https://keras.io/api/layers/convolution_layers/convolution1d/)\n",
    "- Adding or substituting in other types of layers\n",
    "\n",
    "For the full list of layers to try, check the documentation: https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b5966-f575-426b-bdbd-2d60ab061e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
