[
  {
    "objectID": "slides/day2.html#deep-learning",
    "href": "slides/day2.html#deep-learning",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning",
    "text": "Deep learning\n\n\n\n\nThe point of deep learning is to sequentially learn better feature representations, and use these to solve a task.\n\n\n\n\n\ninsufficient:   good:    better:\n\n\n\ndata   data    data\n\n\\(\\rightarrow\\)   \\(\\rightarrow\\)    \\(\\rightarrow\\)\n\nprediction   representation (e.g.¬†crosscorr)   representation (e.g.¬†crosscorr)\n\n  \\(\\rightarrow\\)    \\(\\rightarrow\\)\n\n  prediction    representation (e.g.¬†crosscorr)\n\n  ¬†   \\(\\rightarrow\\)\n\n  ¬†    prediction\n\n\n\n\n\nSince neural networks are universal function approximators, they can model arbitrarily complex relationships. The cost of doing so, is that we need a lot of data."
  },
  {
    "objectID": "slides/day2.html#crosscorrelation",
    "href": "slides/day2.html#crosscorrelation",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nCrosscorrelation of two real-valued functions \\(f\\) and \\(g\\) is defined as\n\\[\n(f \\star g)(\\tau) = \\int_{-\\infty}^{\\infty} f(t)g(t + \\tau)\n\\]\n\nFor discrete signals the operation is computationally simple since \\(\\int \\rightarrow \\sum\\) and we just need to multiply and sum."
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-1",
    "href": "slides/day2.html#crosscorrelation-1",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation"
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-2",
    "href": "slides/day2.html#crosscorrelation-2",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation"
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-3",
    "href": "slides/day2.html#crosscorrelation-3",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation"
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-4",
    "href": "slides/day2.html#crosscorrelation-4",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation"
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-5",
    "href": "slides/day2.html#crosscorrelation-5",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation"
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-6",
    "href": "slides/day2.html#crosscorrelation-6",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation"
  },
  {
    "objectID": "slides/day2.html#crosscorrelation-appreciation-slide",
    "href": "slides/day2.html#crosscorrelation-appreciation-slide",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation appreciation slide",
    "text": "Crosscorrelation appreciation slide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Time-reversing \\(f\\) in the definition of crosscorrelation yields the convolution operation.\nIn ML we got the terms mixed up and only talk about convolution, even though we do crosscorrelation."
  },
  {
    "objectID": "slides/day2.html#some-more-terminology",
    "href": "slides/day2.html#some-more-terminology",
    "title": "NORSAR ML Workshop",
    "section": "Some more terminology",
    "text": "Some more terminology\n\nLayer: A slice of the model where an internal representation is computed\n\n\n\n\n\ndata(input layer)\n\n\\(\\rightarrow\\)\n\nrepresentation(hidden layer 1)\n\n\\(\\rightarrow\\)\n\nrepresentation(hidden layer 2)\n\n\\(\\rightarrow\\)\n\nprediction(output layer)\n\n\n\n\n\n\nFilter: A template (sometimes also called a kernel)\n\n\n\n\nActivation function: Usually applied after each layer to introduce non-linearity to the model\n\n\n\n\n\n\nSigmoid\n\n\n\n\n\ntanh\n\n\n\n\n\nReLU\n\n\n\n\n\nGELU\n\n\n\n\n\nSwish"
  },
  {
    "objectID": "slides/day2.html#some-deep-learning-frameworks",
    "href": "slides/day2.html#some-deep-learning-frameworks",
    "title": "NORSAR ML Workshop",
    "section": "Some deep learning frameworks",
    "text": "Some deep learning frameworks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-level  (optional)\n\n\nCompute  (choose one)\n\n\nSupporting  (useful)"
  },
  {
    "objectID": "slides/day2.html#interlude-programming",
    "href": "slides/day2.html#interlude-programming",
    "title": "NORSAR ML Workshop",
    "section": "Interlude: Programming üë©üèº‚Äçüíª",
    "text": "Interlude: Programming üë©üèº‚Äçüíª"
  },
  {
    "objectID": "slides/day2.html#numpy-numerical-python",
    "href": "slides/day2.html#numpy-numerical-python",
    "title": "NORSAR ML Workshop",
    "section": "NumPy: Numerical Python",
    "text": "NumPy: Numerical Python\n\n\nMost (all?) scientific or data-driven Python projects rely on NumPy.\nThe different deep learning libraries implement the same concepts, so if we know NumPy, the transition to &lt;new flashy DL library‚ú®&gt; is easy.\n\n\n\n\nCore concept: Vectorisation\nPerforming efficent operations on all data at once, without writing explicit loops"
  },
  {
    "objectID": "slides/day2.html#numpy-arrays",
    "href": "slides/day2.html#numpy-arrays",
    "title": "NORSAR ML Workshop",
    "section": "NumPy arrays",
    "text": "NumPy arrays\nAt the core of NumPy is the array, which supports vectorised operations.\n\nExample: Given a list of numbers, make a new list, where all the elements are multiplied by 2.\n\n\n\nVanilla python:\n&gt;&gt;&gt; a = [1,2,3]; b = []\n&gt;&gt;&gt; for elem in a:\n&gt;&gt;&gt;   b.append(elem * 2)\nb\n[2, 4, 6]\n\n\nNumpy:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.array([1,2,3])\n&gt;&gt;&gt; b = a * 2\n&gt;&gt;&gt; b\narray([2 4 6])\n\n\n\nArrays can have any number of dimensions ‚Äì e.g.¬†a 2D matrix is written\n&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; print(a)\n[[1 2 3]\n [4 5 6]]\n&gt;&gt;&gt; a.shape\n(2, 3)"
  },
  {
    "objectID": "slides/day2.html#numpy-arrays-selecting-data",
    "href": "slides/day2.html#numpy-arrays-selecting-data",
    "title": "NORSAR ML Workshop",
    "section": "NumPy arrays: Selecting data",
    "text": "NumPy arrays: Selecting data\nIn normal python, elements are sliced from a list using [start : stop]:\n&gt;&gt;&gt; a = [0, 1, 2, 3, 4]\n&gt;&gt;&gt; a[2:4]                  # (remember zero-based indexing)\n[2, 3]    \n&gt;&gt;&gt; a[:]                    # no start or stop -&gt; select everything\n[0, 1, 2, 3, 4]\n\nNumPy extends this to any number of dimensions, separated by ‚Äú,‚Äù:\n&gt;&gt;&gt; a = np.arange(1,10).reshape(3,3)\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\nSelect single element:\n&gt;&gt;&gt; a[1, 1]\n5\n\n\n\nSelect row:\n&gt;&gt;&gt; a[1, :]\narray([4, 5, 6])\n\n\n\nSelect column:\n&gt;&gt;&gt; a[:, 1]\narray([2, 5, 8])"
  },
  {
    "objectID": "slides/day2.html#numpy-arrays-broadcasting",
    "href": "slides/day2.html#numpy-arrays-broadcasting",
    "title": "NORSAR ML Workshop",
    "section": "NumPy arrays: Broadcasting",
    "text": "NumPy arrays: Broadcasting\nOperations on arrays are typically done element-by-element.\n\n\n&gt;&gt;&gt; a = np.array([1,2,3])\n&gt;&gt;&gt; np.power(a, 2)\narray([1, 4, 9])\n\n\n&gt;&gt;&gt; b = np.array([4,5,6])\n&gt;&gt;&gt; a * b\narray([ 4, 10, 18])\n\n\n\nIn cases the shapes of two arrays don‚Äôt match, numpy will try to make them match(aka broadcasting):\n# I type this\n&gt;&gt;&gt; a * 2\narray([2, 4, 6])\n# ...and numpy does this:\n&gt;&gt;&gt; a * np.array([2, 2, 2])\narray([2, 4, 6])\n\n\n\n\n\nNumpy docs"
  },
  {
    "objectID": "slides/day2.html#shapes",
    "href": "slides/day2.html#shapes",
    "title": "NORSAR ML Workshop",
    "section": "Shapes",
    "text": "Shapes\nThe number of time steps, channels, etc is described by the shape of the array:"
  },
  {
    "objectID": "slides/day2.html#low-level-tensorflow",
    "href": "slides/day2.html#low-level-tensorflow",
    "title": "NORSAR ML Workshop",
    "section": "Low-level TensorFlow",
    "text": "Low-level TensorFlow\nUsually we don‚Äôt need to get involved with the underlying computational framework (in our case TensorFlow). But in case:\nMost things work like NumPy, but with the benefit of GPU support and JIT compilation.\nThe core object is the Tensor, which is basically a multidimensional array.\n\nNumPy\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.array([[1,2,3], [4,5,6]], dtype=np.float32)\n&gt;&gt;&gt; print(x)\n[[1. 2. 3.]\n [4. 5. 6.]]\n\n\nTensorFlow\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; x = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\n&gt;&gt;&gt; print(x)\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)"
  },
  {
    "objectID": "slides/day2.html#building-neural-networks",
    "href": "slides/day2.html#building-neural-networks",
    "title": "NORSAR ML Workshop",
    "section": "Building neural networks",
    "text": "Building neural networks"
  },
  {
    "objectID": "slides/day2.html#my-first-convolutional-network",
    "href": "slides/day2.html#my-first-convolutional-network",
    "title": "NORSAR ML Workshop",
    "section": "My first convolutional network ü¶Ñ",
    "text": "My first convolutional network ü¶Ñ\nYesterday we pieced together a convnet using Keras‚Äô sequential model API:\n\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=(waveforms.shape[1], waveforms.shape[2])),\n        \n        keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'),\n        keras.layers.MaxPooling1D(2),\n        \n        keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'),\n        keras.layers.MaxPooling1D(2),\n        \n        keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'),\n        \n        keras.layers.GlobalMaxPooling1D(),\n        \n        keras.layers.Dense(1, activation='sigmoid')\n    ]\n)"
  },
  {
    "objectID": "slides/day2.html#pattern-hierarchies",
    "href": "slides/day2.html#pattern-hierarchies",
    "title": "NORSAR ML Workshop",
    "section": "Pattern hierarchies",
    "text": "Pattern hierarchies\nRemember the cat:"
  },
  {
    "objectID": "slides/day2.html#pattern-hierarchies-1",
    "href": "slides/day2.html#pattern-hierarchies-1",
    "title": "NORSAR ML Workshop",
    "section": "Pattern hierarchies",
    "text": "Pattern hierarchies\nComputer vision example:\n\n\n\n\n\n\nFrom F. Chollet: Deep Learning with Python"
  },
  {
    "objectID": "slides/day2.html#layer-activations",
    "href": "slides/day2.html#layer-activations",
    "title": "NORSAR ML Workshop",
    "section": "Layer activations",
    "text": "Layer activations\nWe can visualise what each filter does by looking at its activation on the test image: The output after the convolution and applying the activation function.\nExamples:\n\n\n\n\n\n\nLayer 1, filter 4\n\n\n\n\n\n\n\nLayer 1, filter 7"
  },
  {
    "objectID": "slides/day2.html#layer-activations-1",
    "href": "slides/day2.html#layer-activations-1",
    "title": "NORSAR ML Workshop",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 1 and 2:"
  },
  {
    "objectID": "slides/day2.html#layer-activations-2",
    "href": "slides/day2.html#layer-activations-2",
    "title": "NORSAR ML Workshop",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 3:"
  },
  {
    "objectID": "slides/day2.html#layer-activations-3",
    "href": "slides/day2.html#layer-activations-3",
    "title": "NORSAR ML Workshop",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 4 (last):"
  },
  {
    "objectID": "slides/day2.html#keras",
    "href": "slides/day2.html#keras",
    "title": "NORSAR ML Workshop",
    "section": "Keras",
    "text": "Keras\nThe Keras framework contains all the high-level components we need to construct and train a neural network:\n\nkeras.layers: Different types of layers and activation functions\nkeras.callbacks: Monitor, modify or stop the training process\nkeras.optimizers: Optimisation algorithms\nkeras.metrics: Performance metrics\nkeras.losses: Loss functions\nkeras.datasets: Small datasets for testing\nkeras.applications: Pre-trained networks for different tasks"
  },
  {
    "objectID": "slides/day2.html#exercise-number-3",
    "href": "slides/day2.html#exercise-number-3",
    "title": "NORSAR ML Workshop",
    "section": "Exercise number 3",
    "text": "Exercise number 3\n\n\n\nBuilding a phase picker\nOpen in Colab or download"
  },
  {
    "objectID": "slides/day2.html#post-exercise",
    "href": "slides/day2.html#post-exercise",
    "title": "NORSAR ML Workshop",
    "section": "Post-exercise",
    "text": "Post-exercise\nGreat resource for illustrations and comparison of deep learning phase pickers:\nMyklebust, E. B., & K√∂hler, A. (2024). Deep learning models for regional phase detection on seismic stations in Northern Europe and the European Arctic. Geophysical Journal International, 239(2), 862-881."
  },
  {
    "objectID": "slides/day2.html#deep-learning-phase-pickers",
    "href": "slides/day2.html#deep-learning-phase-pickers",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning phase pickers",
    "text": "Deep learning phase pickers\n\n\n\nPhaseNet"
  },
  {
    "objectID": "slides/day2.html#deep-learning-phase-pickers-1",
    "href": "slides/day2.html#deep-learning-phase-pickers-1",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning phase pickers",
    "text": "Deep learning phase pickers\n\n\n\nEPick"
  },
  {
    "objectID": "slides/day2.html#deep-learning-phase-pickers-2",
    "href": "slides/day2.html#deep-learning-phase-pickers-2",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning phase pickers",
    "text": "Deep learning phase pickers\n\n\n\nEQTransformer"
  },
  {
    "objectID": "slides/day2.html#deep-learning-phase-pickers-3",
    "href": "slides/day2.html#deep-learning-phase-pickers-3",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning phase pickers",
    "text": "Deep learning phase pickers\n\n\n\nTPhaseNet"
  },
  {
    "objectID": "slides/day2.html#current-and-future-ml-at-norsar",
    "href": "slides/day2.html#current-and-future-ml-at-norsar",
    "title": "NORSAR ML Workshop",
    "section": "Current and future ML at NORSAR",
    "text": "Current and future ML at NORSAR\n(Andreas K)"
  },
  {
    "objectID": "slides/day2.html#exercise-number-4",
    "href": "slides/day2.html#exercise-number-4",
    "title": "NORSAR ML Workshop",
    "section": "Exercise number 4",
    "text": "Exercise number 4\n\n\n\nUsing pretrained models\nOpen in Colab or download"
  },
  {
    "objectID": "slides/day2.html#tips-for-doing-ml-when-we-have-little-training-data",
    "href": "slides/day2.html#tips-for-doing-ml-when-we-have-little-training-data",
    "title": "NORSAR ML Workshop",
    "section": "Tips for doing ML when we have little training data",
    "text": "Tips for doing ML when we have little training data\n\n\nAugmentation\nArtificially expand the dataset by e.g. randomly adding noise\n\n\n\n\nTransfer learning\nTrain a model on a separate, larger dataset, then carefully re-train (fine-tune) it on the actual data\nCan re-use general patterns learnt in the bigger dataset, and then increase specificity to the actual task afterwards.\n\n\n\n\nUse pretrained models (potentially with fine-tuning)\nVarious models pretrained for seismic data available through SeisBench\nCan also use pretrained models for entirely different tasks, if we reformat our data sufficiently (like in the exercise)"
  },
  {
    "objectID": "slides/day2.html#discussion",
    "href": "slides/day2.html#discussion",
    "title": "NORSAR ML Workshop",
    "section": "Discussion",
    "text": "Discussion\nHow will you use machine learning?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "",
    "text": "Welcome to the 2025 workshop on machine learning (ML) in seismology, where we will learn, try out, and develop ML tools for use at NORSAR.\nSlides, exercises and other material will be kept on this webpage for later reference. If you have suggestions for additions or improvements, submit a pull request!"
  },
  {
    "objectID": "index.html#before-the-workshop",
    "href": "index.html#before-the-workshop",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Before the workshop",
    "text": "Before the workshop\nTo make the workshop as productive as possible, we encourage everyone to go through the following beforehand:\n\nTest that you have a setup that can run the exercises. We aim to run everything online in Google Colab, which requires a Google account, but nothing more. To complete the test, click here to open our first exercise, and click through it (takes 2 min, tops). If you do not have/want a Google account, take a look at the other options listed in Section¬†4.0.3.\nRead some of the background material in Section¬†3, especially the first of the review articles, to get some inspiration to what tasks can be solved with modern machine learning.\nThink about topics or ideas to you would like to discuss or even prototype during the workshop. Around half of day 2 will be dedicated to exploring new projects.\n\n\nOptional\nIn addition to the points above, it will be helpful for the exercises we have planned to take a look at the technical stuff in Section¬†4, and run though the various tutorials to get comfortable with Python and its libraries. We will do a quickstart on the first day of the workshop as well, but doing it beforehand is a lot more efficient."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Agenda",
    "text": "Agenda\nBoth days will take place in Maskinhallen. The agenda will most likely change a bit up until Wednesday, but the start and end times are absolute.\nRemote participation: Teams link can be found in the meeting invitation.\n\nWednesday\n\n\n\n9.00\nIntro\nSlides\n\n\n9.30\nExercise: Event detection\nNotebook: Colab or download\n\n\n10.00\nPost-exercise: Understanding what we did\nSlides\n\n\n11.00\nExercise: Event classification\nNotebook: Colab or download\n\n\n11.45\n‚ú®Special‚ú® lunch\n\n\n\n12.30\nDiscussion\n\n\n\n13.00\nEnd of day 1\n\n\n\n\n\n\nThursday\n\n\n\n9.00\nRecap from yesterday\n\n\n\n9.15\nDeep learning tools\nSlides\n\n\n9.30\nExercise: Training deep learning models\nNotebook: Colab or download\n\n\n10.00\nPost-exercise: Finding the optimal model\nSlides\n\n\n10.30\nRecent and future ML at NORSAR\n\n\n\n11.00\nExercise: Using pretrained models\nNotebook: Colab or download\n\n\n11.45\nLunch (kantinen)\n\n\n\n12.30\nWrap-up and discussions\n\n\n\n13.00\nEnd of day 2"
  },
  {
    "objectID": "index.html#sec-background",
    "href": "index.html#sec-background",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Background material",
    "text": "Background material\nThese review articles give a fairly comprehensive (although brief) overview of machine learning applications in seismology:\n\nOld but gold: Machine Learning in Seismology: Turning Data into Insights, Q. Kong et al., 2019 (direct link to PDF)\nMore recent, with relatively narrow scope: Machine Learning in Earthquake Seismology, S.M. Mousavi and G. C. Beroza, 2023 (direct link to PDF)\nRelatively recent, with wider scope: Deep-learning seismology, S.M. Mousavi and G.C. Beroza, 2022\n\nFor a beginner-friendly, hands-on book on deep learning, Deep Learning with Python by F. Chollet is an absolutely great resource.\n\nSelected NORSAR ML works\nAs you know, NORSAR has made strong contributions to the field, documented for instance by these works:\n\nDeep learning models for regional phase detection on seismic stations in Northern Europe and the European Arctic (code available on GitHub)\nSelf-supervised learning of seismological data reveals new eruptive sequences at the Mayotte submarine volcano\nMonitoring urban construction and quarry blasts with low-cost seismic sensors and deep learning tools in the city of Oslo, Norway\nArrayNet: A Combined Seismic Phase Classification and Back‚ÄêAzimuth Regression Neural Network for Array Processing Pipelines (code available on GitHub)\nPredicting infrasound transmission loss using deep learning\nSeismic and infrasound monitoring of military conflicts using machine learning\nEnhancing seismic calving event identification in Svalbard through empirical matched field processing and machine learning\n\nNote that we have a bit of a tradition for open-sourcing the code used to produce results, made available on the NORSAR GitHub and the Zenodo platform."
  },
  {
    "objectID": "index.html#sec-technical",
    "href": "index.html#sec-technical",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Technical things",
    "text": "Technical things\nDuring the workshop we will do various hands-on exercises, most of which benefit from some experience with programming. Luckily, the de-facto programming language for machine learning is Python, which is relatively easy to get started with.\nThere are three pieces of tech we will be using:\n\nPython, the programming langage,\nPython libraries, containing code to actually do stuff, and\nJupyter notebooks, which is a convenient way of running Python.\n\nThe magic of 3. is that you do not need to install anything, instead we run the code online, in a browser. Below follows an introduction, and some reference material, to the three.\n\nPython\nIf you already have some coding background in a different language, the Python cheat sheet gives the essential overview of the language, while the official tutorial goes in depth.\nIf you have less programming experience, Google‚Äôs Python course is quite nice, or in case you would like something in video format, Microsoft has made a Python for beginners video series.\nHowever, in the end we can get away with limited Python knowledge, since the majority of our interaction with both the data and the ML models is through the set of libraries decribed below. And, given the modern tools at our disposal, we can always ask for help.\n\nHey ChatGPT, I already know Matlab, can you give me a super-short (like, really short) introduction to Python?\n\n\n\nRequired Python libraries\nThe main data structure we use for data-intensive tasks like ML is the array, provided by the NumPy (Numerical Python) library. It looks like this:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.array([[1,2,3], [4,5,6]])\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6]])\n&gt;&gt;&gt; a.shape\n(2, 3)\nand allows for performing operations on each element without writing explicit for-loops:\n&gt;&gt;&gt; a + 10\narray([[11, 12, 13],\n       [14, 15, 16]])\nWhile we will cover some NumPy basics during the workshop, the essentials are given in these two tutorials: NumPy quickstart, for those who are already familiar with Python, and NumPy basics for beginners for ‚Ä¶ well, beginners.\nTo perform all the computations involved in doing deep learning, we need a library that can do automatic differentiation and efficient matrix multiplications. A popular option is TensorFlow and its companion interface for building neural networks, Keras. A nice thing about Keras is the extensive example gallery, which serves as inspiration for solving loads of different tasks. While other options for deep learning frameworks have their benefits too, they mostly all follow similar syntax for data operations as NumPy, making it relatively easy to switch between them, as long as one knows NumPy.\n\n\nJupyter notebooks\nJupyter notebooks form a convenient way of prototyping code, by mixing code, text and graphics in a single document. The easiest option for running the exercise notebooks is through a cloud service, in which case there is nothing to install. Alternatives are (choose one):\n\nGoogle Colab (Preferred): Requires a Google account (like GMail), but otherwise free.\nKaggle Code: Requires an account, but otherwise free.\nInstall on our own machine: No accounts required, but setup may take a few minutes. Instructions are given in the Jupyter docs.\n\nThe notebooks are mostly self-explanatory, but the basics are also given in the Jupyter Notebook 101 course, and documented in detail on the Jupyter website."
  },
  {
    "objectID": "index.html#post-read-and-other-material",
    "href": "index.html#post-read-and-other-material",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Post-read and other material",
    "text": "Post-read and other material\nHere we will collect workshop material for future reference üíæ."
  },
  {
    "objectID": "slides/day1.html#agenda",
    "href": "slides/day1.html#agenda",
    "title": "NORSAR ML Workshop",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\nToday\n\n\n\n9.00\nIntro\n\n\n\n9.30\nExercise: Event detection\n\n\n\n10.00\nPost-exercise: Understanding what we did\n\n\n\n11.00\nExercise: Event classification\n\n\n\n11.45\n‚ú®Special‚ú® lunch\n\n\n\n12.30\nDiscussion\n\n\n\n13.00\nEnd of day 1\n\n\n\n\n\nTomorrow\n\n\n\n9.00\nRecap from yesterday\n\n\n9.15\nDeep learning tools\n\n\n9.30\nExercise: Training deep learning models\n\n\n10.00\nPost-exercise: Finding the optimal model\n\n\n10.30\nRecent and future ML at NORSAR\n\n\n11.00\nHackathon\n\n\n11.45\nLunch (kantinen)\n\n\n12.30\nWrap-up and discussions\n\n\n13.00\nEnd of day 2"
  },
  {
    "objectID": "slides/day1.html#what-can-you-do-with-machine-learning",
    "href": "slides/day1.html#what-can-you-do-with-machine-learning",
    "title": "NORSAR ML Workshop",
    "section": "What can you do with machine learning?",
    "text": "What can you do with machine learning?\n\n\n\n(outside of seismology)"
  },
  {
    "objectID": "slides/day1.html#the-what",
    "href": "slides/day1.html#the-what",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\n\n\n\nArtificial intelligence (AI): Umbrella term for computer systems that make smart decisions\nMachine learning (ML): Collection of algorithms that learn to recognise patterns in data\nDeep learning: ML that recognises complex patterns, using neural networks \n\nThe big AI tools of today are driven by advancements in\n\nDeep learning ‚Äì bigger and better models\nComputing ‚Äì bigger and better processors"
  },
  {
    "objectID": "slides/day1.html#the-what-1",
    "href": "slides/day1.html#the-what-1",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nTraditional approach: Symbolic or rule-based AI\n\n\n\n\nIF amplitude &gt; threshold AND duration &gt; 2 seconds\nTHEN earthquake;\nELSE noise;\n\n\nMachine learning approach:\n\n\n\n\n\n\n\nThis is an earthquake\n\n\nThis is not\n\n\nCompute a function to separate the two."
  },
  {
    "objectID": "slides/day1.html#the-what-2",
    "href": "slides/day1.html#the-what-2",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nNumber of results on Google Scholar per year:"
  },
  {
    "objectID": "slides/day1.html#task-that-have-been-solved-with-ml",
    "href": "slides/day1.html#task-that-have-been-solved-with-ml",
    "title": "NORSAR ML Workshop",
    "section": "Task that have been solved with ML",
    "text": "Task that have been solved with ML\n\nEvent discrimination, such as\n\n\nEarthquake vs explosion\n\n\n\n\nEarthquake vs glacier calving\n\n\n\n\n\nPhase picking\n\n\n\n\nPolarity determination\n\n\n\n\nPhase association\n\n\n\n\n\n\nLinville et al., Geophys. Res. Lett. 46:3643‚Äì51\n\n\n\n\n\n\n\nK√∂hler et al., Geophys. J. Int. 230:1305‚Äì17\n\n\n\n\n\n\n\nK√∂hler et al., Geophys. J. Int. 239:862‚Äì81\n\n\n\n\n\n\n\nUchide T., Geophys. J. Int. 223:1658‚Äì71\n\n\n\n\n\n\n\nDickey et al., Seismol. Res. Lett. 91:356‚Äì69"
  },
  {
    "objectID": "slides/day1.html#task-that-have-been-solved-with-ml-1",
    "href": "slides/day1.html#task-that-have-been-solved-with-ml-1",
    "title": "NORSAR ML Workshop",
    "section": "Task that have been solved with ML",
    "text": "Task that have been solved with ML\n\nSource parametrisation\n\n\nBackazimuth estimation\n\n\n\n\nLocalisation\n\n\n\n\nFocal mechanism estimation\n\n\n\n\n\nSeismogram simulation\n\n\n\n\nGround motion characterisation\n\n\n\n\nEvent clustering\n\n\n\n\n\n\nK√∂hler et al., Bull. Seismol. Soc. Am. 113.6 (2023): 2345-62\n\n\n\n\n\n\n\nZhang et al., Geophys. J. Int. 241:1853-67\n\n\n\n\n\n\n\nSteinberg et al., J. Geophys. Res. Solid Earth 126:e2021JB022685\n\n\n\n\n\n\n\nMoseley et al., Solid Earth 11:1527‚Äì49\n\n\n\n\n\n\n\nM√ºnchmeyer at al., Geophys. J. Int. 225::646‚Äì56\n\n\n\n\n\n\n\nSnover at al., Seismol. Soc. Am. 92::1011‚Äì22"
  },
  {
    "objectID": "slides/day1.html#deep-learning-seismology-papers",
    "href": "slides/day1.html#deep-learning-seismology-papers",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning seismology papers",
    "text": "Deep learning seismology papers\n\n\n\n\n\n\n\nhttps://smousavi05.github.io/dl_seismology/"
  },
  {
    "objectID": "slides/day1.html#section-8",
    "href": "slides/day1.html#section-8",
    "title": "NORSAR ML Workshop",
    "section": "",
    "text": "DL papers byuse case\n\n\nhttps://smousavi05.github.io/dl_seismology/"
  },
  {
    "objectID": "slides/day1.html#section-9",
    "href": "slides/day1.html#section-9",
    "title": "NORSAR ML Workshop",
    "section": "",
    "text": "DL papers bymodel type\n\n\nhttps://smousavi05.github.io/dl_seismology/"
  },
  {
    "objectID": "slides/day1.html#the-what-3",
    "href": "slides/day1.html#the-what-3",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nSome terminology:\n\nModel: A mathematical function (or algorithm) that takes data in and gives predictions out\nParameters: The internal variables of the model\nLabel: The correct answer that the model should learn to predict\nSupervised learning: Learning the relation between data and labels\nUnsupervised learning: Learning patterns without explicit labels\n\nWe also need to know some statistics, but let‚Äôs deal with that later."
  },
  {
    "objectID": "slides/day1.html#the-what-4",
    "href": "slides/day1.html#the-what-4",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nToday and tomorrow:\nIntro to modern ML technologies, which is mainly pattern recognition.\n\n\n\n\n2006 ü•±\n\n\n\n\n\n\n\n2023 ü§©"
  },
  {
    "objectID": "slides/day1.html#the-why",
    "href": "slides/day1.html#the-why",
    "title": "NORSAR ML Workshop",
    "section": "The why",
    "text": "The why\nModern ML tech is accessible!\n\n\n\nThe major frameworks are\n\nopen source, and\n(relatively) easy to use."
  },
  {
    "objectID": "slides/day1.html#the-wow",
    "href": "slides/day1.html#the-wow",
    "title": "NORSAR ML Workshop",
    "section": "The wow",
    "text": "The wow\n\n\n\n\n\nMousavi, S. M., & Beroza, G. C. (2023). Machine learning in earthquake seismology. Annu. Rev.¬†Earth Planet. Sci., 51(1), 105-129."
  },
  {
    "objectID": "slides/day1.html#open-datasets",
    "href": "slides/day1.html#open-datasets",
    "title": "NORSAR ML Workshop",
    "section": "Open datasets",
    "text": "Open datasets\nSome readily available datasets suited for ML:\n\nSTEAD: 1.2M 3C waveforms from 450k local earthquakes\nINSTANCE: 1.3M 3C waveforms from 54k local and regional earthquakes\nCREW: 1.6M waveforms from regional earthquakes\nMLAAPDE: 5.1M waveforms from local to teleseismic events\n\n(others exist too)\n\n\n\nPrepped NORSAR catalog for regional events recorded at ARCES: Zenodo"
  },
  {
    "objectID": "slides/day1.html#exercise-number-1",
    "href": "slides/day1.html#exercise-number-1",
    "title": "NORSAR ML Workshop",
    "section": "Exercise number 1",
    "text": "Exercise number 1\n\n\n\nComparing detection methods\nOpen in Colab or download"
  },
  {
    "objectID": "slides/day1.html#post-exercise",
    "href": "slides/day1.html#post-exercise",
    "title": "NORSAR ML Workshop",
    "section": "Post-exercise",
    "text": "Post-exercise\nUnderstanding what we did"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation",
    "href": "slides/day1.html#crosscorrelation",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nTemplate identical to signal"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation-1",
    "href": "slides/day1.html#crosscorrelation-1",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nTemplate not identical to signal"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation-2",
    "href": "slides/day1.html#crosscorrelation-2",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nShort template"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation-3",
    "href": "slides/day1.html#crosscorrelation-3",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nMultiple short templates"
  },
  {
    "objectID": "slides/day1.html#transitioning-into-ml",
    "href": "slides/day1.html#transitioning-into-ml",
    "title": "NORSAR ML Workshop",
    "section": "Transitioning into ML",
    "text": "Transitioning into ML\n\n\n\nWe‚Äôll pursure three ideas:\n\n\n\n\nMultiple, short templates\n\n\n\n\n\nDoing correlation on top of the output from correlation\n\nDeep learning\n\n\n\n\n\n\nLearn optimal templates, rather than selecting explicit ones\n\nDeep learning"
  },
  {
    "objectID": "slides/day1.html#deep-learning",
    "href": "slides/day1.html#deep-learning",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning",
    "text": "Deep learning\n\n\n\n\nThe point of deep learning is to sequentially learn better feature representations, and use these to solve a task.\n\n\n\n\n\n\ninsufficient:  \n\n\ngood:   \n\n\nbetter:\n\n\n\n\n\ndata  \n\n\ndata   \n\n\ndata\n\n\n\n\\(\\rightarrow\\)  \n\n\n\\(\\rightarrow\\)   \n\n\n\\(\\rightarrow\\)\n\n\n\nprediction  \n\n\nrepresentation (e.g.¬†crosscorr)  \n\n\nrepresentation (e.g.¬†crosscorr)\n\n\n\n \n\\(\\rightarrow\\)   \n\n\n\\(\\rightarrow\\)\n\n\n\n \nprediction   \n\n\nrepresentation (e.g.¬†crosscorr)\n\n\n\n \n¬†   \n\\(\\rightarrow\\)\n\n\n\n \n¬†   \nprediction"
  },
  {
    "objectID": "slides/day1.html#section-10",
    "href": "slides/day1.html#section-10",
    "title": "NORSAR ML Workshop",
    "section": "",
    "text": "TensorFlowplayground"
  },
  {
    "objectID": "slides/day1.html#pattern-hierarchies",
    "href": "slides/day1.html#pattern-hierarchies",
    "title": "NORSAR ML Workshop",
    "section": "Pattern hierarchies",
    "text": "Pattern hierarchies\n\n\n\n\nDetails tomorrow"
  },
  {
    "objectID": "slides/day1.html#training",
    "href": "slides/day1.html#training",
    "title": "NORSAR ML Workshop",
    "section": "Training",
    "text": "Training\n\n\nOptimal choice of model parameters can usually not be found analytically\n\\(\\rightarrow\\) need to iteratively search for it, which we call training the model.\n\nDeep learning models are essentially composite, differentiable functions, meaning we can use gradient descent.\n\n\n\n\n\nGood: DL libraries do the differentiation for us!\n\n\nBad: It‚Äôs computationally expensive\n\n\nGood: Modern hardware (GPUs) are very efficient at this (also, it could have been worse)\n\n\n\n\n\\[\n\\small\n\\nabla \\color{MediumVioletRed}{L}(\\color{teal}{\\boldsymbol{\\theta}}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_0} \\\\\n  \\vdots \\\\\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_n} \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/day1.html#hardware-acceleration",
    "href": "slides/day1.html#hardware-acceleration",
    "title": "NORSAR ML Workshop",
    "section": "Hardware acceleration",
    "text": "Hardware acceleration\n\n\nDeep learning training and inference is considerably faster on a graphics processing unit (GPU)\nFor the next exercises we can enable it in Colab by selecting\nRuntime \\(\\rightarrow\\) Change runtime type \\(\\rightarrow\\) T4 GPU\n\n\n\nThe NORSAR GPU server is available at\nssh gpu.norsar.no\n\n\n\n\n\nNVDA"
  },
  {
    "objectID": "slides/day1.html#micro-break",
    "href": "slides/day1.html#micro-break",
    "title": "NORSAR ML Workshop",
    "section": "Micro-break üèñ",
    "text": "Micro-break üèñ"
  },
  {
    "objectID": "slides/day1.html#selecting-hyperparameters",
    "href": "slides/day1.html#selecting-hyperparameters",
    "title": "NORSAR ML Workshop",
    "section": "Selecting hyperparameters",
    "text": "Selecting hyperparameters"
  },
  {
    "objectID": "slides/day1.html#evaluating-models",
    "href": "slides/day1.html#evaluating-models",
    "title": "NORSAR ML Workshop",
    "section": "Evaluating models",
    "text": "Evaluating models\nEvaluating a model should be done on an independent data set (we want to know how well it performs on new, unseen data)\nTypically we set aside a part of the data, and use this only for final evaluation.\n\n\n\n\n\n# \"X\" are the data, \"y\" are the targets.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)"
  },
  {
    "objectID": "slides/day1.html#comparing-models",
    "href": "slides/day1.html#comparing-models",
    "title": "NORSAR ML Workshop",
    "section": "Comparing models",
    "text": "Comparing models\nModel selection\n\nIn case we want to compare different models, we need a third set:  The validation set\nThe test set is still only for final evaluation\n\n\n\n\n\n\n\nML models are prone to overfitting ‚Äì i.e.¬†memorising the training data.\nHow do we know if (when) this happens?\n\nCan compare performance on the training set to the validation set"
  },
  {
    "objectID": "slides/day1.html#exercise-number-2",
    "href": "slides/day1.html#exercise-number-2",
    "title": "NORSAR ML Workshop",
    "section": "Exercise number 2",
    "text": "Exercise number 2\n\n\n\nTraining an earthquake classifier\nOpen in Colab or download"
  },
  {
    "objectID": "slides/day1.html#post-exercise-1",
    "href": "slides/day1.html#post-exercise-1",
    "title": "NORSAR ML Workshop",
    "section": "Post-exercise",
    "text": "Post-exercise"
  },
  {
    "objectID": "slides/day1.html#constructing-a-deep-learning-model",
    "href": "slides/day1.html#constructing-a-deep-learning-model",
    "title": "NORSAR ML Workshop",
    "section": "Constructing a deep learning model",
    "text": "Constructing a deep learning model\nmodel.summary()\n\n\n\nModel: \"sequential\"\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ conv1d (Conv1D)                 ‚îÇ (None, 5998, 16)       ‚îÇ            64 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ max_pooling1d (MaxPooling1D)    ‚îÇ (None, 2999, 16)       ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_1 (Conv1D)               ‚îÇ (None, 2997, 16)       ‚îÇ           784 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ max_pooling1d_1 (MaxPooling1D)  ‚îÇ (None, 1498, 16)       ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_2 (Conv1D)               ‚îÇ (None, 1496, 16)       ‚îÇ           784 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ global_max_pooling1d            ‚îÇ (None, 16)             ‚îÇ             0 ‚îÇ\n‚îÇ (GlobalMaxPooling1D)            ‚îÇ                        ‚îÇ               ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense (Dense)                   ‚îÇ (None, 1)              ‚îÇ            17 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n Total params: 1,649 (6.44 KB)\n\n Trainable params: 1,649 (6.44 KB)\n\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "slides/day1.html#the-joys-of-training-a-model",
    "href": "slides/day1.html#the-joys-of-training-a-model",
    "title": "NORSAR ML Workshop",
    "section": "The joys of training a model",
    "text": "The joys of training a model\nRecall gradient descent:"
  },
  {
    "objectID": "slides/day1.html#the-joys-of-training-a-model-1",
    "href": "slides/day1.html#the-joys-of-training-a-model-1",
    "title": "NORSAR ML Workshop",
    "section": "The joys of training a model",
    "text": "The joys of training a model\nRecall gradient descent:\n\n\nLocalminimum -&gt; bad predictions"
  },
  {
    "objectID": "slides/day1.html#the-joys-of-training-a-model-2",
    "href": "slides/day1.html#the-joys-of-training-a-model-2",
    "title": "NORSAR ML Workshop",
    "section": "The joys of training a model",
    "text": "The joys of training a model\nRecall gradient descent:\n\n\nLocalminimum -&gt; bad predictions\n\n\nPlateau -&gt; slow convergence"
  },
  {
    "objectID": "slides/day1.html#going-further-phase-picking",
    "href": "slides/day1.html#going-further-phase-picking",
    "title": "NORSAR ML Workshop",
    "section": "Going further: Phase picking",
    "text": "Going further: Phase picking\nA yes or no classification in a time window is a little simplistic\nFor an automated system we rather want phase picks:\n\nThen: How to define the labels?"
  },
  {
    "objectID": "slides/day1.html#going-further-phase-picking-1",
    "href": "slides/day1.html#going-further-phase-picking-1",
    "title": "NORSAR ML Workshop",
    "section": "Going further: Phase picking",
    "text": "Going further: Phase picking\nReformat the labels into 3-component time series:\n\n\\(\\rightarrow\\) More on this tomorrow\n\nZhu, W., & Beroza, G. C. (2019). PhaseNet: a deep-neural-network-based seismic arrival-time picking method. Geophysical Journal International, 216(1), 261-273."
  },
  {
    "objectID": "slides/day1.html#end-of-day-one",
    "href": "slides/day1.html#end-of-day-one",
    "title": "NORSAR ML Workshop",
    "section": "End of day one",
    "text": "End of day one"
  }
]