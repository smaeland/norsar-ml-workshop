[
  {
    "objectID": "slides/day2.html#current-seismology-sota",
    "href": "slides/day2.html#current-seismology-sota",
    "title": "NORSAR ML Workshop",
    "section": "Current seismology SOTA",
    "text": "Current seismology SOTA"
  },
  {
    "objectID": "slides/day2.html#lol",
    "href": "slides/day2.html#lol",
    "title": "NORSAR ML Workshop",
    "section": "LOL",
    "text": "LOL\nOur short templates can be “re-used” to detect similar but distinct patterns\nHowever, most interesting patterns are considerably longer than the templates.\n-&gt; Want a way to compose small patterns into larger ones"
  },
  {
    "objectID": "slides/day2.html#training",
    "href": "slides/day2.html#training",
    "title": "NORSAR ML Workshop",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "slides/day2.html#exercise-1",
    "href": "slides/day2.html#exercise-1",
    "title": "NORSAR ML Workshop",
    "section": "Exercise 1",
    "text": "Exercise 1\nFirst: Repeat phase picksing from yesterday"
  },
  {
    "objectID": "slides/day2.html#ml-for-waveform-data",
    "href": "slides/day2.html#ml-for-waveform-data",
    "title": "NORSAR ML Workshop",
    "section": "ML for waveform data",
    "text": "ML for waveform data\nCross-correlation\nConvolutions"
  },
  {
    "objectID": "slides/day2.html#neural-networks",
    "href": "slides/day2.html#neural-networks",
    "title": "NORSAR ML Workshop",
    "section": "Neural networks",
    "text": "Neural networks\nSequentially improved features"
  },
  {
    "objectID": "slides/day2.html#deep-learning",
    "href": "slides/day2.html#deep-learning",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning",
    "text": "Deep learning\n\n\n\nThe point of deep learning is to sequentially learn better feature representations, and use these to solve a task.\n\n\n\nSince neural networks are universal function approximators, they can model arbitrarily complex relationships. The cost of doing so, is that we need a lot of data."
  },
  {
    "objectID": "slides/day2.html#enter-the-convolution-operation",
    "href": "slides/day2.html#enter-the-convolution-operation",
    "title": "NORSAR ML Workshop",
    "section": "Enter the convolution operation",
    "text": "Enter the convolution operation\nThe foundation for modern computer vision (plus lots of other things!) is convolution:\nan operation that takes in two functions and returns a new function\n\n\n\n\n\n\\[\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\n\\]"
  },
  {
    "objectID": "slides/day2.html#enter-the-convolution-operation-1",
    "href": "slides/day2.html#enter-the-convolution-operation-1",
    "title": "NORSAR ML Workshop",
    "section": "Enter the convolution operation",
    "text": "Enter the convolution operation\nThe foundation for modern computer vision (plus lots of other things!) is convolution:\nan operation that takes in two functions and returns a new function\n\n\n\n\n\n\\[\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\n\\]\n\n\n\nIn practice, convolution is a way to recognise and localise patterns in data"
  },
  {
    "objectID": "slides/day2.html#discrete-convolution",
    "href": "slides/day2.html#discrete-convolution",
    "title": "NORSAR ML Workshop",
    "section": "Discrete convolution",
    "text": "Discrete convolution\nConvolution is a lot easier with discrete data such as images, because:\n\nthe integral becomes a sum\nthe first function is our image\nthe second function is our kernel or filter, which tries to find patterns in the image."
  },
  {
    "objectID": "slides/day2.html#convolutions-recap",
    "href": "slides/day2.html#convolutions-recap",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/day2.html#convolutions-recap-1",
    "href": "slides/day2.html#convolutions-recap-1",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/day2.html#convolutions-recap-2",
    "href": "slides/day2.html#convolutions-recap-2",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/day2.html#convolutions-recap-3",
    "href": "slides/day2.html#convolutions-recap-3",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/day2.html#convolutions-recap-4",
    "href": "slides/day2.html#convolutions-recap-4",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/day2.html#convolutions-recap-5",
    "href": "slides/day2.html#convolutions-recap-5",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/day2.html#convolutions-detour",
    "href": "slides/day2.html#convolutions-detour",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions detour",
    "text": "Convolutions detour\nIf the take the convolution operation\n\\[\n\\small\n(f \\ast g)(t) \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\n\\]\nbut reverse one of the functions (\\(\\small f(t) \\to f(-t)\\)), we get the similar operation called cross-correlation:\n\n\\[\n\\small\nf \\star g \\equiv f(-t) \\ast g(t)\n\\]"
  },
  {
    "objectID": "slides/day2.html#convolutions-detour-1",
    "href": "slides/day2.html#convolutions-detour-1",
    "title": "NORSAR ML Workshop",
    "section": "Convolutions detour",
    "text": "Convolutions detour\nIf the take the convolution operation\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\n\\]\nbut reverse one of the functions (\\(\\small f(t) \\to f(-t)\\)), we get the similar operation called cross-correlation:\n\n\\[\n\\small\nf \\star g \\equiv f(-t) \\ast g(t)\n\\]"
  },
  {
    "objectID": "slides/day2.html#break",
    "href": "slides/day2.html#break",
    "title": "NORSAR ML Workshop",
    "section": "Break",
    "text": "Break"
  },
  {
    "objectID": "slides/day2.html#seismology-tasks-solved-with-ml-current-sota",
    "href": "slides/day2.html#seismology-tasks-solved-with-ml-current-sota",
    "title": "NORSAR ML Workshop",
    "section": "Seismology tasks solved with ML / current SOTA",
    "text": "Seismology tasks solved with ML / current SOTA"
  },
  {
    "objectID": "slides/day2.html#exercise-2",
    "href": "slides/day2.html#exercise-2",
    "title": "NORSAR ML Workshop",
    "section": "Exercise 2",
    "text": "Exercise 2\nTrain an ML pick detector"
  },
  {
    "objectID": "slides/day2.html#post-exercise",
    "href": "slides/day2.html#post-exercise",
    "title": "NORSAR ML Workshop",
    "section": "Post-exercise",
    "text": "Post-exercise"
  },
  {
    "objectID": "slides/day2.html#augmentation",
    "href": "slides/day2.html#augmentation",
    "title": "NORSAR ML Workshop",
    "section": "Augmentation",
    "text": "Augmentation\nMove to day 2"
  },
  {
    "objectID": "slides/day2.html#day-2",
    "href": "slides/day2.html#day-2",
    "title": "NORSAR ML Workshop",
    "section": "DAY 2",
    "text": "DAY 2"
  },
  {
    "objectID": "slides/day2.html#recap-from-day-1",
    "href": "slides/day2.html#recap-from-day-1",
    "title": "NORSAR ML Workshop",
    "section": "Recap from day 1",
    "text": "Recap from day 1"
  },
  {
    "objectID": "slides/day2.html#the-more-advanced-stuff",
    "href": "slides/day2.html#the-more-advanced-stuff",
    "title": "NORSAR ML Workshop",
    "section": "The more advanced stuff",
    "text": "The more advanced stuff\nDeep learning components"
  },
  {
    "objectID": "slides/day2.html#libraries-for-machine-learning",
    "href": "slides/day2.html#libraries-for-machine-learning",
    "title": "NORSAR ML Workshop",
    "section": "Libraries for machine learning",
    "text": "Libraries for machine learning"
  },
  {
    "objectID": "slides/day2.html#python-libraries",
    "href": "slides/day2.html#python-libraries",
    "title": "NORSAR ML Workshop",
    "section": "Python libraries",
    "text": "Python libraries\n\n\n\n\n\n\n\n\nhttps://numpy.org/\n\n\nnumpy provides fast manipulation of large arrays and matrices\n\n\n\n\n\n\nhttps://scikit-learn.org/\n\n\nscikit-learn has a big selection of machine learning models and functions for data processing and evaluation"
  },
  {
    "objectID": "slides/day2.html#numpy-arrays",
    "href": "slides/day2.html#numpy-arrays",
    "title": "NORSAR ML Workshop",
    "section": "numpy arrays",
    "text": "numpy arrays\nThe core of numpy is the array, on which we can do operations without explicit loops:\nExample: Given a list of numbers, make a new list, where all the elements are multiplied by 2.\n\n\nVanilla python:\n&gt;&gt;&gt; a = [1,2,3]; b = []\n&gt;&gt;&gt; for elem in a:\n&gt;&gt;&gt;   b.append(elem * 2)\nb\n[2, 4, 6]\n\n\nNumpy:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.array([1,2,3])\n&gt;&gt;&gt; b = a * 2\n&gt;&gt;&gt; b\narray([2 4 6])\n\n\n\nArrays can have any number of dimensions – e.g. a 2D matrix is written\n&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; print(a)\n[[1 2 3]\n [4 5 6]]\n&gt;&gt;&gt; a.ndim\n2\n&gt;&gt;&gt; a.shape\n(2, 3)"
  },
  {
    "objectID": "slides/day2.html#numpy-arrays-1",
    "href": "slides/day2.html#numpy-arrays-1",
    "title": "NORSAR ML Workshop",
    "section": "numpy arrays",
    "text": "numpy arrays\nIn normal python, elements are sliced from a list using [start : stop]:\n&gt;&gt;&gt; a = [0, 1, 2, 3, 4]\n&gt;&gt;&gt; a[2:4]                  # (remember zero-based indexing)\n[2, 3]    \n&gt;&gt;&gt; a[:]                    # no start or stop -&gt; select everything\n[0, 1, 2, 3, 4]\n\nNumpy extends this to any number of dimensions, separated by ,\n&gt;&gt;&gt; a = np.arange(1,10).reshape(3,3)\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\nSelect single element:\n&gt;&gt;&gt; a[1, 1]\n5\n\n\n\nSelect row:\n&gt;&gt;&gt; a[1, :]\narray([4, 5, 6])\n\n\n\nSelect column:\n&gt;&gt;&gt; a[:, 1]\narray([2, 5, 8])"
  },
  {
    "objectID": "slides/day2.html#numpy-arrays-2",
    "href": "slides/day2.html#numpy-arrays-2",
    "title": "NORSAR ML Workshop",
    "section": "numpy arrays",
    "text": "numpy arrays\nOperations on arrays are typically done element-by-element.\n\n\n&gt;&gt;&gt; a = np.array([1,2,3])\n&gt;&gt;&gt; np.power(a, 2)\narray([1, 4, 9])\n\n\n&gt;&gt;&gt; b = np.array([4,5,6])\n&gt;&gt;&gt; a * b\narray([ 4, 10, 18])\n\n\n\nIn cases the shapes of two arrays don’t match, numpy will try to make them match(aka broadcasting):\n# I type this\n&gt;&gt;&gt; a * 2\narray([2, 4, 6])\n# ...and numpy does this:\n&gt;&gt;&gt; a * np.array([2, 2, 2])\narray([2, 4, 6])\n\n\n\n\n\nNumpy docs"
  },
  {
    "objectID": "slides/day2.html#shapes",
    "href": "slides/day2.html#shapes",
    "title": "NORSAR ML Workshop",
    "section": "Shapes",
    "text": "Shapes\nBroadcasting_ works like in NumPy:\n&gt;&gt;&gt; x = tf.constant([1,2,3], dtype=tf.float32)\n&gt;&gt;&gt; x + 1\n&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 3., 4.], dtype=float32)&gt;\nSame for shapes:"
  },
  {
    "objectID": "slides/day2.html#deep-learning-frameworks",
    "href": "slides/day2.html#deep-learning-frameworks",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning frameworks",
    "text": "Deep learning frameworks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-level\n\n\nCompute\n\n\nSupporting"
  },
  {
    "objectID": "slides/day2.html#low-level-tensorflow",
    "href": "slides/day2.html#low-level-tensorflow",
    "title": "NORSAR ML Workshop",
    "section": "Low-level TensorFlow",
    "text": "Low-level TensorFlow\nUsually we don’t need to get involved with TensorFlow. But in case:\nMost things work like NumPy, but with the benefit of GPU support and JIT compilation.\nThe core object is the Tensor, which is basically a multidimensional array.\n\nNumPy\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.array([[1,2,3], [4,5,6]], dtype=np.float32)\n&gt;&gt;&gt; print(x)\n[[1. 2. 3.]\n [4. 5. 6.]]\n\n\nTensorFlow\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; x = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\n&gt;&gt;&gt; print(x)\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)"
  },
  {
    "objectID": "slides/day2.html#keras",
    "href": "slides/day2.html#keras",
    "title": "NORSAR ML Workshop",
    "section": "Keras",
    "text": "Keras\nThe Keras framework contains all the high-level components we need to construct and train a neural network:\n\nkeras.layers: Different types of layers and activation functions\nkeras.callbacks: Monitor, modify or stop the training process\nkeras.optimizers: Optimisation algorithms\nkeras.metrics: Performance metrics\nkeras.losses: Loss functions\nkeras.datasets: Small datasets for testing\nkeras.applications: Pre-trained networks for different tasks"
  },
  {
    "objectID": "slides/day2.html#my-first-convolutional-network",
    "href": "slides/day2.html#my-first-convolutional-network",
    "title": "NORSAR ML Workshop",
    "section": "My first convolutional network ✨",
    "text": "My first convolutional network ✨\nLet’s piece together a convnet using Keras’ sequential model API:\n\n\n\nconvnet = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Flatten(),\n        keras.layers.Dense(10, activation=\"softmax\"),\n    ]\n)\n\n\n\n(More details about activations and training next week)"
  },
  {
    "objectID": "slides/day2.html#my-first-convolutional-network-1",
    "href": "slides/day2.html#my-first-convolutional-network-1",
    "title": "NORSAR ML Workshop",
    "section": "My first convolutional network ✨",
    "text": "My first convolutional network ✨\nconvnet.summary()\nModel: \"sequential_1\"\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 26, 26, 32)          │             320 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 13, 13, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 11, 11, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 5, 5, 32)            │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (Flatten)                  │ (None, 800)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 800)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 10)                  │           8,010 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n Total params: 17,578 (68.66 KB)\n\n Trainable params: 17,578 (68.66 KB)\n\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "slides/day2.html#my-first-convolutional-network-2",
    "href": "slides/day2.html#my-first-convolutional-network-2",
    "title": "NORSAR ML Workshop",
    "section": "My first convolutional network ✨",
    "text": "My first convolutional network ✨\nConfigure the training objective and strategy:\nconvnet.compile(\n  loss=\"categorical_crossentropy\",\n  optimizer=\"adam\",\n  metrics=[\"accuracy\"]\n)\n(again, more details next week)\nStart training!\nconvnet.fit(\n  X_train,\n  y_train,\n  batch_size=128,\n  epochs=15,\n  validation_split=0.1\n)"
  },
  {
    "objectID": "slides/day2.html#decomposition-into-simple-patters-theory-vs-practice",
    "href": "slides/day2.html#decomposition-into-simple-patters-theory-vs-practice",
    "title": "NORSAR ML Workshop",
    "section": "Decomposition into simple patters: Theory vs practice",
    "text": "Decomposition into simple patters: Theory vs practice\nRemember the cat:\n\n\n\n\n\n(We’ll try to classify pictures of cats in exercise 3, but let’s test out a cat detector convnet already now)"
  },
  {
    "objectID": "slides/day2.html#putting-together-an-improved-network",
    "href": "slides/day2.html#putting-together-an-improved-network",
    "title": "NORSAR ML Workshop",
    "section": "Putting together an improved network",
    "text": "Putting together an improved network"
  },
  {
    "objectID": "slides/day2.html#pretrained-models",
    "href": "slides/day2.html#pretrained-models",
    "title": "NORSAR ML Workshop",
    "section": "Pretrained models",
    "text": "Pretrained models"
  },
  {
    "objectID": "slides/day2.html#exercise-unsupervised-learning-with-pretrained-image-models",
    "href": "slides/day2.html#exercise-unsupervised-learning-with-pretrained-image-models",
    "title": "NORSAR ML Workshop",
    "section": "Exercise: Unsupervised learning with pretrained image models?",
    "text": "Exercise: Unsupervised learning with pretrained image models?"
  },
  {
    "objectID": "slides/day2.html#sota-research",
    "href": "slides/day2.html#sota-research",
    "title": "NORSAR ML Workshop",
    "section": "SOTA research",
    "text": "SOTA research\nhttps://www.nature.com/articles/s41467-020-17591-w/figures/3"
  },
  {
    "objectID": "slides/day2.html#norsar-developments",
    "href": "slides/day2.html#norsar-developments",
    "title": "NORSAR ML Workshop",
    "section": "NORSAR developments",
    "text": "NORSAR developments"
  },
  {
    "objectID": "slides/day2.html#future-directions-at-norsar",
    "href": "slides/day2.html#future-directions-at-norsar",
    "title": "NORSAR ML Workshop",
    "section": "Future directions at NORSAR",
    "text": "Future directions at NORSAR\nArrays"
  },
  {
    "objectID": "slides/day2.html#hackathon",
    "href": "slides/day2.html#hackathon",
    "title": "NORSAR ML Workshop",
    "section": "Hackathon",
    "text": "Hackathon"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "",
    "text": "Welcome to the 2025 workshop on machine learning (ML) in seismology, where we will learn, try out, and develop ML tools for use at NORSAR.\nSlides, exercises and other material will be kept on this webpage for later reference. If you have suggestions for additions or improvements, submit a pull request!"
  },
  {
    "objectID": "index.html#before-the-workshop",
    "href": "index.html#before-the-workshop",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Before the workshop",
    "text": "Before the workshop\nTo make the workshop as productive as possible, we encourage everyone to go through the following beforehand:\n\nTest that you have a setup that can run the exercises. We aim to run everything online in Google Colab, which requires a Google account, but nothing more. To complete the test, click here to open our first exercise, and click through it (takes 2 min, tops). If you do not have/want a Google account, take a look at the other options listed in Section 4.0.3.\nRead some of the background material in Section 3, especially the first of the review articles, to get some inspiration to what tasks can be solved with modern machine learning.\nThink about topics or ideas to you would like to discuss or even prototype during the workshop. Around half of day 2 will be dedicated to exploring new projects.\n\n\nOptional\nIn addition to the points above, it will be helpful for the exercises we have planned to take a look at the technical stuff in Section 4, and run though the various tutorials to get comfortable with Python and its libraries. We will do a quickstart on the first day of the workshop as well, but doing it beforehand is a lot more efficient."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Agenda",
    "text": "Agenda\nBoth days will take place in Maskinhallen. The agenda will most likely change a bit up until Wednesday, but the start and end times are absolute.\nRemote participation: Teams link can be found in the meeting invitation.\n\nWednesday\n\n\n\n9.00\nIntro\nSlides\n\n\n9.30\nExercise: Event detection\nNotebook: Colab or download\n\n\n10.00\nPost-exercise: Understanding what we did\nSlides: coming\n\n\n11.00\nExercise: Event classification\nNotebook: Colab or download\n\n\n11.45\n✨Special✨ lunch\n\n\n\n12.30\nDiscussion\n\n\n\n13.00\nEnd of day 1\n\n\n\n\n\n\nThursday\n\n\n\n9.00\nRecap from yesterday\n\n\n\n9.15\nDeep learning tools\nSlides: coming\n\n\n9.30\nExercise: Training deep learning models\nNotebook: coming\n\n\n10.00\nPost-exercise: Finding the optimal model\nSlides: coming\n\n\n10.30\nRecent and future ML at NORSAR\n\n\n\n11.00\nHackathon\n\n\n\n11.45\nLunch (kantinen)\n\n\n\n12.30\nWrap-up and discussions\n\n\n\n13.00\nEnd of day 2"
  },
  {
    "objectID": "index.html#sec-background",
    "href": "index.html#sec-background",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Background material",
    "text": "Background material\nThese review articles give a fairly comprehensive (although brief) overview of machine learning applications in seismology:\n\nOld but gold: Machine Learning in Seismology: Turning Data into Insights, Q. Kong et al., 2019 (direct link to PDF)\nMore recent, with relatively narrow scope: Machine Learning in Earthquake Seismology, S.M. Mousavi and G. C. Beroza, 2023 (direct link to PDF)\nRelatively recent, with wider scope: Deep-learning seismology, S.M. Mousavi and G.C. Beroza, 2022\n\nFor a beginner-friendly, hands-on book on deep learning, Deep Learning with Python by F. Chollet is an absolutely great resource.\n\nSelected NORSAR ML works\nAs you know, NORSAR has made strong contributions to the field, documented for instance by these works:\n\nDeep learning models for regional phase detection on seismic stations in Northern Europe and the European Arctic (code available on GitHub)\nSelf-supervised learning of seismological data reveals new eruptive sequences at the Mayotte submarine volcano\nMonitoring urban construction and quarry blasts with low-cost seismic sensors and deep learning tools in the city of Oslo, Norway\nArrayNet: A Combined Seismic Phase Classification and Back‐Azimuth Regression Neural Network for Array Processing Pipelines (code available on GitHub)\nPredicting infrasound transmission loss using deep learning\nSeismic and infrasound monitoring of military conflicts using machine learning\nEnhancing seismic calving event identification in Svalbard through empirical matched field processing and machine learning\n\nNote that we have a bit of a tradition for open-sourcing the code used to produce results, made available on the NORSAR GitHub and the Zenodo platform."
  },
  {
    "objectID": "index.html#sec-technical",
    "href": "index.html#sec-technical",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Technical things",
    "text": "Technical things\nDuring the workshop we will do various hands-on exercises, most of which benefit from some experience with programming. Luckily, the de-facto programming language for machine learning is Python, which is relatively easy to get started with.\nThere are three pieces of tech we will be using:\n\nPython, the programming langage,\nPython libraries, containing code to actually do stuff, and\nJupyter notebooks, which is a convenient way of running Python.\n\nThe magic of 3. is that you do not need to install anything, instead we run the code online, in a browser. Below follows an introduction, and some reference material, to the three.\n\nPython\nIf you already have some coding background in a different language, the Python cheat sheet gives the essential overview of the language, while the official tutorial goes in depth.\nIf you have less programming experience, Google’s Python course is quite nice, or in case you would like something in video format, Microsoft has made a Python for beginners video series.\nHowever, in the end we can get away with limited Python knowledge, since the majority of our interaction with both the data and the ML models is through the set of libraries decribed below. And, given the modern tools at our disposal, we can always ask for help.\n\nHey ChatGPT, I already know Matlab, can you give me a super-short (like, really short) introduction to Python?\n\n\n\nRequired Python libraries\nThe main data structure we use for data-intensive tasks like ML is the array, provided by the NumPy (Numerical Python) library. It looks like this:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.array([[1,2,3], [4,5,6]])\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6]])\n&gt;&gt;&gt; a.shape\n(2, 3)\nand allows for performing operations on each element without writing explicit for-loops:\n&gt;&gt;&gt; a + 10\narray([[11, 12, 13],\n       [14, 15, 16]])\nWhile we will cover some NumPy basics during the workshop, the essentials are given in these two tutorials: NumPy quickstart, for those who are already familiar with Python, and NumPy basics for beginners for … well, beginners.\nTo perform all the computations involved in doing deep learning, we need a library that can do automatic differentiation and efficient matrix multiplications. A popular option is TensorFlow and its companion interface for building neural networks, Keras. A nice thing about Keras is the extensive example gallery, which serves as inspiration for solving loads of different tasks. While other options for deep learning frameworks have their benefits too, they mostly all follow similar syntax for data operations as NumPy, making it relatively easy to switch between them, as long as one knows NumPy.\n\n\nJupyter notebooks\nJupyter notebooks form a convenient way of prototyping code, by mixing code, text and graphics in a single document. The easiest option for running the exercise notebooks is through a cloud service, in which case there is nothing to install. Alternatives are (choose one):\n\nGoogle Colab (Preferred): Requires a Google account (like GMail), but otherwise free.\nKaggle Code: Requires an account, but otherwise free.\nInstall on our own machine: No accounts required, but setup may take a few minutes. Instructions are given in the Jupyter docs.\n\nThe notebooks are mostly self-explanatory, but the basics are also given in the Jupyter Notebook 101 course, and documented in detail on the Jupyter website."
  },
  {
    "objectID": "index.html#post-read-and-other-material",
    "href": "index.html#post-read-and-other-material",
    "title": "NORSAR Machine Learning Workshop,Sept 17-18 2025",
    "section": "Post-read and other material",
    "text": "Post-read and other material\nHere we will collect workshop material for future reference 💾."
  },
  {
    "objectID": "slides/day1.html#agenda",
    "href": "slides/day1.html#agenda",
    "title": "NORSAR ML Workshop",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\nToday\n\n\n\n9.00\nIntro\n\n\n\n9.30\nExercise: Event detection\n\n\n\n10.00\nPost-exercise: Understanding what we did\n\n\n\n11.00\nExercise: Event classification\n\n\n\n11.45\n✨Special✨ lunch\n\n\n\n12.30\nDiscussion\n\n\n\n13.00\nEnd of day 1\n\n\n\n\n\nTomorrow\n\n\n\n9.00\nRecap from yesterday\n\n\n9.15\nDeep learning tools\n\n\n9.30\nExercise: Training deep learning models\n\n\n10.00\nPost-exercise: Finding the optimal model\n\n\n10.30\nRecent and future ML at NORSAR\n\n\n11.00\nHackathon\n\n\n11.45\nLunch (kantinen)\n\n\n12.30\nWrap-up and discussions\n\n\n13.00\nEnd of day 2"
  },
  {
    "objectID": "slides/day1.html#what-can-you-do-with-machine-learning",
    "href": "slides/day1.html#what-can-you-do-with-machine-learning",
    "title": "NORSAR ML Workshop",
    "section": "What can you do with machine learning?",
    "text": "What can you do with machine learning?\n\n\n\n(outside of seismology)"
  },
  {
    "objectID": "slides/day1.html#the-what",
    "href": "slides/day1.html#the-what",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\n\n\n\nArtificial intelligence (AI): Umbrella term for computer systems that make smart decisions\nMachine learning (ML): Collection of algorithms that learn to recognise patterns in data\nDeep learning: ML that recognises complex patterns, using neural networks \n\nThe big AI tools of today are driven by advancements in\n\nDeep learning – bigger and better models\nComputing – bigger and better processors"
  },
  {
    "objectID": "slides/day1.html#the-what-1",
    "href": "slides/day1.html#the-what-1",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nTraditional approach: Symbolic or rule-based AI\n\n\n\n\nIF amplitude &gt; threshold AND duration &gt; 2 seconds\nTHEN earthquake;\nELSE noise;\n\n\nMachine learning approach:\n\n\n\n\n\n\n\nThis is an earthquake\n\n\nThis is not\n\n\nCompute a function to separate the two."
  },
  {
    "objectID": "slides/day1.html#the-what-2",
    "href": "slides/day1.html#the-what-2",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nNumber of results on Google Scholar per year:"
  },
  {
    "objectID": "slides/day1.html#task-that-have-been-solved-with-ml",
    "href": "slides/day1.html#task-that-have-been-solved-with-ml",
    "title": "NORSAR ML Workshop",
    "section": "Task that have been solved with ML",
    "text": "Task that have been solved with ML\n\nEvent discrimination, such as\n\n\nEarthquake vs explosion\n\n\n\n\nEarthquake vs glacier calving\n\n\n\n\n\nPhase picking\n\n\n\n\nPolarity determination\n\n\n\n\nPhase association\n\n\n\n\n\n\nLinville et al., Geophys. Res. Lett. 46:3643–51\n\n\n\n\n\n\n\nKöhler et al., Geophys. J. Int. 230:1305–17\n\n\n\n\n\n\n\nKöhler et al., Geophys. J. Int. 239:862–81\n\n\n\n\n\n\n\nUchide T., Geophys. J. Int. 223:1658–71\n\n\n\n\n\n\n\nDickey et al., Seismol. Res. Lett. 91:356–69"
  },
  {
    "objectID": "slides/day1.html#task-that-have-been-solved-with-ml-1",
    "href": "slides/day1.html#task-that-have-been-solved-with-ml-1",
    "title": "NORSAR ML Workshop",
    "section": "Task that have been solved with ML",
    "text": "Task that have been solved with ML\n\nSource parametrisation\n\n\nBackazimuth estimation\n\n\n\n\nLocalisation\n\n\n\n\nFocal mechanism estimation\n\n\n\n\n\nSeismogram simulation\n\n\n\n\nGround motion characterisation\n\n\n\n\nEvent clustering\n\n\n\n\n\n\nKöhler et al., Bull. Seismol. Soc. Am. 113.6 (2023): 2345-62\n\n\n\n\n\n\n\nZhang et al., Geophys. J. Int. 241:1853-67\n\n\n\n\n\n\n\nSteinberg et al., J. Geophys. Res. Solid Earth 126:e2021JB022685\n\n\n\n\n\n\n\nMoseley et al., Solid Earth 11:1527–49\n\n\n\n\n\n\n\nMünchmeyer at al., Geophys. J. Int. 225::646–56\n\n\n\n\n\n\n\nSnover at al., Seismol. Soc. Am. 92::1011–22"
  },
  {
    "objectID": "slides/day1.html#deep-learning-seismology-papers",
    "href": "slides/day1.html#deep-learning-seismology-papers",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning seismology papers",
    "text": "Deep learning seismology papers\n\n\n\n\n\n\n\nhttps://smousavi05.github.io/dl_seismology/"
  },
  {
    "objectID": "slides/day1.html#section-8",
    "href": "slides/day1.html#section-8",
    "title": "NORSAR ML Workshop",
    "section": "",
    "text": "DL papers byuse case\n\n\nhttps://smousavi05.github.io/dl_seismology/"
  },
  {
    "objectID": "slides/day1.html#section-9",
    "href": "slides/day1.html#section-9",
    "title": "NORSAR ML Workshop",
    "section": "",
    "text": "DL papers bymodel type\n\n\nhttps://smousavi05.github.io/dl_seismology/"
  },
  {
    "objectID": "slides/day1.html#the-what-3",
    "href": "slides/day1.html#the-what-3",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nSome terminology:\n\nModel: A mathematical function (or algorithm) that takes data in and gives predictions out\nParameters: The internal variables of the model\nLabel: The correct answer that the model should learn to predict\nSupervised learning: Learning the relation between data and labels\nUnsupervised learning: Learning patterns without explicit labels\n\nWe also need to know some statistics, but let’s deal with that later."
  },
  {
    "objectID": "slides/day1.html#the-what-4",
    "href": "slides/day1.html#the-what-4",
    "title": "NORSAR ML Workshop",
    "section": "The what",
    "text": "The what\nToday and tomorrow:\nIntro to modern ML technologies, which is mainly pattern recognition.\n\n\n\n\n2006 🥱\n\n\n\n\n\n\n\n2023 🤩"
  },
  {
    "objectID": "slides/day1.html#the-why",
    "href": "slides/day1.html#the-why",
    "title": "NORSAR ML Workshop",
    "section": "The why",
    "text": "The why\nModern ML tech is accessible!\n\n\n\nThe major frameworks are\n\nopen source, and\n(relatively) easy to use."
  },
  {
    "objectID": "slides/day1.html#the-wow",
    "href": "slides/day1.html#the-wow",
    "title": "NORSAR ML Workshop",
    "section": "The wow",
    "text": "The wow\n\n\n\n\n\nMousavi, S. M., & Beroza, G. C. (2023). Machine learning in earthquake seismology. Annu. Rev. Earth Planet. Sci., 51(1), 105-129."
  },
  {
    "objectID": "slides/day1.html#open-datasets",
    "href": "slides/day1.html#open-datasets",
    "title": "NORSAR ML Workshop",
    "section": "Open datasets",
    "text": "Open datasets\nSome readily available datasets suited for ML:\n\nSTEAD: 1.2M 3C waveforms from 450k local earthquakes\nINSTANCE: 1.3M 3C waveforms from 54k local and regional earthquakes\nCREW: 1.6M waveforms from regional earthquakes\nMLAAPDE: 5.1M waveforms from local to teleseismic events\n\n(others exist too)\n\n\n\nPrepped NORSAR catalog for regional events recorded at ARCES: Zenodo"
  },
  {
    "objectID": "slides/day1.html#exercise-number-1",
    "href": "slides/day1.html#exercise-number-1",
    "title": "NORSAR ML Workshop",
    "section": "Exercise number 1",
    "text": "Exercise number 1\n\n\n\nComparing detection methods\nOpen in Colab or download"
  },
  {
    "objectID": "slides/day1.html#post-exercise",
    "href": "slides/day1.html#post-exercise",
    "title": "NORSAR ML Workshop",
    "section": "Post-exercise",
    "text": "Post-exercise\nUnderstanding what we did"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation",
    "href": "slides/day1.html#crosscorrelation",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nTemplate identical to signal"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation-1",
    "href": "slides/day1.html#crosscorrelation-1",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nTemplate not identical to signal"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation-2",
    "href": "slides/day1.html#crosscorrelation-2",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nShort template"
  },
  {
    "objectID": "slides/day1.html#crosscorrelation-3",
    "href": "slides/day1.html#crosscorrelation-3",
    "title": "NORSAR ML Workshop",
    "section": "Crosscorrelation",
    "text": "Crosscorrelation\nMultiple short templates"
  },
  {
    "objectID": "slides/day1.html#transitioning-into-ml",
    "href": "slides/day1.html#transitioning-into-ml",
    "title": "NORSAR ML Workshop",
    "section": "Transitioning into ML",
    "text": "Transitioning into ML\n\n\n\nWe’ll pursure three ideas:\n\n\n\n\nMultiple, short templates\n\n\n\n\n\nDoing correlation on top of the output from correlation\n\nDeep learning\n\n\n\n\n\n\nLearn optimal templates, rather than selecting explicit ones\n\nDeep learning"
  },
  {
    "objectID": "slides/day1.html#deep-learning",
    "href": "slides/day1.html#deep-learning",
    "title": "NORSAR ML Workshop",
    "section": "Deep learning",
    "text": "Deep learning\n\n\n\n\nThe point of deep learning is to sequentially learn better feature representations, and use these to solve a task.\n\n\n\n\n\n\ninsufficient:  \n\n\ngood:   \n\n\nbetter:\n\n\n\n\n\ndata  \n\n\ndata   \n\n\ndata\n\n\n\n\\(\\rightarrow\\)  \n\n\n\\(\\rightarrow\\)   \n\n\n\\(\\rightarrow\\)\n\n\n\nprediction  \n\n\nrepresentation (e.g. crosscorr)  \n\n\nrepresentation (e.g. crosscorr)\n\n\n\n \n\\(\\rightarrow\\)   \n\n\n\\(\\rightarrow\\)\n\n\n\n \nprediction   \n\n\nrepresentation (e.g. crosscorr)\n\n\n\n \n    \n\\(\\rightarrow\\)\n\n\n\n \n    \nprediction"
  },
  {
    "objectID": "slides/day1.html#section-10",
    "href": "slides/day1.html#section-10",
    "title": "NORSAR ML Workshop",
    "section": "",
    "text": "TensorFlowplayground"
  },
  {
    "objectID": "slides/day1.html#pattern-hierarchies",
    "href": "slides/day1.html#pattern-hierarchies",
    "title": "NORSAR ML Workshop",
    "section": "Pattern hierarchies",
    "text": "Pattern hierarchies\n\n\n\n\nDetails tomorrow"
  },
  {
    "objectID": "slides/day1.html#training",
    "href": "slides/day1.html#training",
    "title": "NORSAR ML Workshop",
    "section": "Training",
    "text": "Training\n\n\nOptimal choice of model parameters can usually not be found analytically\n\\(\\rightarrow\\) need to iteratively search for it, which we call training the model.\n\nDeep learning models are essentially composite, differentiable functions, meaning we can use gradient descent.\n\n\n\n\n\nGood: DL libraries do the differentiation for us!\n\n\nBad: It’s computationally expensive\n\n\nGood: Modern hardware (GPUs) are very efficient at this (also, it could have been worse)\n\n\n\n\n\\[\n\\small\n\\nabla \\color{MediumVioletRed}{L}(\\color{teal}{\\boldsymbol{\\theta}}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_0} \\\\\n  \\vdots \\\\\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_n} \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/day1.html#hardware-acceleration",
    "href": "slides/day1.html#hardware-acceleration",
    "title": "NORSAR ML Workshop",
    "section": "Hardware acceleration",
    "text": "Hardware acceleration\n\n\nDeep learning training and inference is considerably faster on a graphics processing unit (GPU)\nFor the next exercises we can enable it in Colab by selecting\nRuntime \\(\\rightarrow\\) Change runtime type \\(\\rightarrow\\) T4 GPU\n\n\n\nThe NORSAR GPU server is available at\nssh gpu.norsar.no\n\n\n\n\n\nNVDA"
  },
  {
    "objectID": "slides/day1.html#micro-break",
    "href": "slides/day1.html#micro-break",
    "title": "NORSAR ML Workshop",
    "section": "Micro-break 🏖",
    "text": "Micro-break 🏖"
  },
  {
    "objectID": "slides/day1.html#selecting-hyperparameters",
    "href": "slides/day1.html#selecting-hyperparameters",
    "title": "NORSAR ML Workshop",
    "section": "Selecting hyperparameters",
    "text": "Selecting hyperparameters"
  },
  {
    "objectID": "slides/day1.html#evaluating-models",
    "href": "slides/day1.html#evaluating-models",
    "title": "NORSAR ML Workshop",
    "section": "Evaluating models",
    "text": "Evaluating models\nEvaluating a model should be done on an independent data set (we want to know how well it performs on new, unseen data)\nTypically we set aside a part of the data, and use this only for final evaluation.\n\n\n\n\n\n# \"X\" are the data, \"y\" are the targets.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)"
  },
  {
    "objectID": "slides/day1.html#comparing-models",
    "href": "slides/day1.html#comparing-models",
    "title": "NORSAR ML Workshop",
    "section": "Comparing models",
    "text": "Comparing models\nModel selection\n\nIn case we want to compare different models, we need a third set:  The validation set\nThe test set is still only for final evaluation\n\n\n\n\n\n\n\nML models are prone to overfitting – i.e. memorising the training data.\nHow do we know if (when) this happens?\n\nCan compare performance on the training set to the validation set"
  },
  {
    "objectID": "slides/day1.html#exercise-number-2",
    "href": "slides/day1.html#exercise-number-2",
    "title": "NORSAR ML Workshop",
    "section": "Exercise number 2",
    "text": "Exercise number 2\n\n\n\nTraining an earthquake classifier\nOpen in Colab or download"
  },
  {
    "objectID": "slides/day1.html#post-exercise-1",
    "href": "slides/day1.html#post-exercise-1",
    "title": "NORSAR ML Workshop",
    "section": "Post-exercise",
    "text": "Post-exercise"
  },
  {
    "objectID": "slides/day1.html#constructing-a-deep-learning-model",
    "href": "slides/day1.html#constructing-a-deep-learning-model",
    "title": "NORSAR ML Workshop",
    "section": "Constructing a deep learning model",
    "text": "Constructing a deep learning model\nmodel.summary\n-&gt; look at keras.io"
  },
  {
    "objectID": "slides/day1.html#the-joys-of-training-a-model",
    "href": "slides/day1.html#the-joys-of-training-a-model",
    "title": "NORSAR ML Workshop",
    "section": "The joys of training a model",
    "text": "The joys of training a model\nRecall gradient descent:"
  },
  {
    "objectID": "slides/day1.html#the-joys-of-training-a-model-1",
    "href": "slides/day1.html#the-joys-of-training-a-model-1",
    "title": "NORSAR ML Workshop",
    "section": "The joys of training a model",
    "text": "The joys of training a model\nRecall gradient descent:\n\n\nLocalminimum -&gt; bad predictions"
  },
  {
    "objectID": "slides/day1.html#the-joys-of-training-a-model-2",
    "href": "slides/day1.html#the-joys-of-training-a-model-2",
    "title": "NORSAR ML Workshop",
    "section": "The joys of training a model",
    "text": "The joys of training a model\nRecall gradient descent:\n\n\nLocalminimum -&gt; bad predictions\n\n\nPlateau -&gt; slow convergence"
  },
  {
    "objectID": "slides/day1.html#going-further-phase-picking",
    "href": "slides/day1.html#going-further-phase-picking",
    "title": "NORSAR ML Workshop",
    "section": "Going further: Phase picking",
    "text": "Going further: Phase picking\nA yes or no classification in a time window is a little simplistic\nFor an automated system we rather want phase picks:\n\nThen: How to define the labels?"
  },
  {
    "objectID": "slides/day1.html#going-further-phase-picking-1",
    "href": "slides/day1.html#going-further-phase-picking-1",
    "title": "NORSAR ML Workshop",
    "section": "Going further: Phase picking",
    "text": "Going further: Phase picking\nReformat the labels into 3-component time series:\n\n\\(\\rightarrow\\) More on this tomorrow\n\nZhu, W., & Beroza, G. C. (2019). PhaseNet: a deep-neural-network-based seismic arrival-time picking method. Geophysical Journal International, 216(1), 261-273."
  },
  {
    "objectID": "slides/day1.html#end-of-day-one",
    "href": "slides/day1.html#end-of-day-one",
    "title": "NORSAR ML Workshop",
    "section": "End of day one",
    "text": "End of day one"
  }
]