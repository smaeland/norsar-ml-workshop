{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4646a9ee-eb0a-48d9-b15a-49edd257a517",
   "metadata": {},
   "source": [
    "# Phase picker neural network\n",
    "\n",
    "It's time to up our earthquake detection game and not only do binary classification, but add phase picking capabilities. Following the approach of the [PhaseNet](https://academic.oup.com/gji/article/216/1/261/5129142) model from 2018, improved on by Andreas K and Erik last year with the [TPhaseNet](https://academic.oup.com/gji/article/239/2/862/7740467) model, we'll be a bit clever on how we encode the target labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72fcd2-b71f-4a77-87cc-3960ff988a1b",
   "metadata": {},
   "source": [
    "But first, download some data. Again we use a selection of events from the STEAD dataset, but we're not too interested in pure-noise waveforms this time, so we use training and testing files containing only events with identified P and S arrival times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd537da-b781-4334-9393-e4af695ce2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/norsar-ml-ws/events_phases_Zonly_TRAIN.h5\n",
    "! wget https://storage.googleapis.com/norsar-ml-ws/events_phases_Zonly_TEST.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae95739-9dd4-4cdb-ac4c-d4b021758c17",
   "metadata": {},
   "source": [
    "Import the usual suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17422f8e-7a53-4e52-8490-5f1698a723a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf24902-178a-4350-b8ce-04878c14e4c0",
   "metadata": {},
   "source": [
    "## Create target labels\n",
    "\n",
    "As shown on today's slides, a good way of framing the phase picking problem is to make the labels be a time series too, which at any given time shows whether the seismogram contains the beginning of a P arrival, the beginning of an S arrival, or no arrival at all -- the latter we'll refer to as noise. \n",
    "\n",
    "Ideally, our target timeseries for a recorded earthquake will then contain almost only 'noise' labels, exept for the exact microsecond of the P and S arrivals. This is a little strict, so we can allow ourselves to instead insert the arrival labels as distributions, centred around the labelled arrival time. In a couple of cells below, we'll show graphically how this works. First, let's just write the relevant code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b237d-6da6-4b8a-88c4-8d6107ec61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This thing is a *generator* -- everytime it's called, it returns a new event.\n",
    "class Hdf5DataGenerator:\n",
    "\n",
    "    def __call__(self, filename, batchsize, normalise=True):\n",
    "\n",
    "        if isinstance(filename, bytes):\n",
    "            filename = filename.decode()    # Because of technical reasons\n",
    "\n",
    "        # Open file, get datasets\n",
    "        with h5py.File(filename, \"r\") as fin:\n",
    "\n",
    "            waveforms = fin.get('waveforms')\n",
    "            event_types = fin.get('type')\n",
    "            p_start = fin.get('p_start')\n",
    "            s_start = fin.get('s_start')\n",
    "\n",
    "            waveform_length = waveforms[0].shape[0]\n",
    "            istart = 0\n",
    "            istop = batchsize\n",
    "            exhausted = False\n",
    "\n",
    "            # This is where we create the distribution around the pick\n",
    "            pick_width = 100   # equals 1 sec\n",
    "            half_pick_width = pick_width // 2\n",
    "            pick = scipy.signal.windows.gaussian(pick_width, 12)\n",
    "\n",
    "            while not exhausted:\n",
    "\n",
    "                # Load a batch (= group) of data\n",
    "                data = waveforms[istart:istop]\n",
    "                targets = []\n",
    "\n",
    "                if normalise:\n",
    "                    max_vals = np.max(np.abs(data), axis=1, keepdims=True)\n",
    "                    data /= (max_vals + 1e-8)\n",
    "\n",
    "                # Create the target class waveforms \n",
    "                for i in range(len(data)):\n",
    "\n",
    "                    p_true = np.zeros(shape=(waveform_length))    # P pick, set to [0.0, 0.0, 0.0, ...]\n",
    "                    s_true = np.zeros(shape=(waveform_length))    # S pick, set to [0.0, 0.0, 0.0, ...]\n",
    "                    n_true = np.ones(shape=(waveform_length))     #  Noise, set to [1.0, 1.0, 1.0, ...]\n",
    "\n",
    "                    p_pos = p_start[istart + i]\n",
    "                    s_pos = s_start[istart + i]\n",
    "\n",
    "                    # Ensure there is a valid pick                    \n",
    "                    if p_pos > half_pick_width and s_pos > half_pick_width:\n",
    "                    \n",
    "                        # Insert pick \n",
    "                        p_true[p_pos - half_pick_width : p_pos + half_pick_width] = pick\n",
    "                        s_true[s_pos - half_pick_width : s_pos + half_pick_width] = pick\n",
    "                        \n",
    "                        n_true -= p_true\n",
    "                        n_true -= s_true\n",
    "\n",
    "                    targets.append(\n",
    "                        np.dstack([p_true, s_true, n_true])\n",
    "                    )\n",
    "\n",
    "                # Return this batch of data (and then continue)\n",
    "                yield (data, np.vstack(targets))\n",
    "\n",
    "                istart += batchsize\n",
    "                istop += batchsize\n",
    "\n",
    "                # No more events in file.\n",
    "                if istop > len(waveforms):\n",
    "                    exhausted = True\n",
    "                    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdae83d-9919-4e59-84db-2f4033c274a0",
   "metadata": {},
   "source": [
    "Let't try out the fancy generator thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3267683-f802-4edc-9da2-97927f2dc7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate it\n",
    "gen = Hdf5DataGenerator()\n",
    "\n",
    "# Now we can loop over batches\n",
    "for batch in gen('events_phases_Zonly_TRAIN.h5', 1):\n",
    "\n",
    "    batch_data = batch[0]\n",
    "    batch_targets = batch[1]\n",
    "    \n",
    "    print('batch_data.shape:', batch_data.shape)\n",
    "    print('batch_targets.shape:', batch_targets.shape)\n",
    "\n",
    "    # Plot the first event in the batch:\n",
    "    data = batch_data[0]\n",
    "    targets = batch_targets[0]\n",
    "\n",
    "    \n",
    "    _, ax = plt.subplots(4, 1, sharex=True, figsize=(12, 8))\n",
    "    xvals = np.arange(data.shape[0])\n",
    "    \n",
    "    ax[0].plot(xvals, data[:, 0])\n",
    "    ax[1].plot(xvals, targets[:, 0], color='blue')\n",
    "    ax[2].plot(xvals, targets[:, 1], color='red')\n",
    "    ax[3].plot(xvals, targets[:, 2], color='green')\n",
    "    \n",
    "    ax[0].set_ylabel('Waveform')\n",
    "    ax[1].set_ylabel('P arrival')\n",
    "    ax[2].set_ylabel('S arrival')\n",
    "    ax[3].set_ylabel('Noise')\n",
    "    \n",
    "\n",
    "    break    # Ok let's stop it here already "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0fbf8-3655-4fbb-94d9-9fd82226ec9b",
   "metadata": {},
   "source": [
    "Have a look at the plot and convince yourself if this makes conceptual sense or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e8799-744e-4d1d-ac78-352237dea3d9",
   "metadata": {},
   "source": [
    "## Make a `tf.data.Dataset` (for performance)\n",
    "\n",
    "Last time we saw that loading all data into memory at once _could_ lead to some problems, and this time around, it surely will. For this reason we'd like to load only one _batch_ at a time, train on it, and then remove it from memory to make room for the next batch.\n",
    "\n",
    "Here we can also make the realisation that while training on the GPU, the CPU is mostly idle in-between loading batches. So we can in fact use it to construct the label timeseries on the fly -- while one batch is training, we can run the above code simultaneously to prepare the next batch, instead of sequentially first to produce all the label timeseries, and then start training. This is why writing the above code as a _generator_ was a useful thing.\n",
    "\n",
    "But how do we schedule when to start processing which batch? Luckily TensorFlow can do this part for us, if we put everything in a `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d52636-bc31-4787-810f-8f91f11b073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on a batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Input file\n",
    "filename_train = 'events_phases_Zonly_TRAIN.h5'\n",
    "\n",
    "# Now build the Dataset:\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    Hdf5DataGenerator(),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 6000, 1), dtype=tf.float32, name='data'),\n",
    "        tf.TensorSpec(shape=(None, 6000, 3), dtype=tf.float32, name='targets')\n",
    "    ), \n",
    "    args=(filename_train, batch_size)\n",
    ")\n",
    "\n",
    "# Tell TensorFlow to prepare batches in parallell during training\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c6555-5e52-453d-93b4-16ccb5a179c5",
   "metadata": {},
   "source": [
    "Cool. To check that it does the job, it's always nice to make a plot, for visual verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dcd1b-5b39-4d96-a473-1e849f9e5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can loop over batches in the dataset\n",
    "for batch_data, batch_target in train_dataset:\n",
    "\n",
    "    # Get the first event in the batch\n",
    "    data = batch_data[0]\n",
    "    target = batch_target[0]\n",
    "\n",
    "    # Check the shapes\n",
    "    print('data.shape:', data.shape)\n",
    "    print('target.shape:', target.shape)\n",
    "\n",
    "    # Make a plot\n",
    "    _, ax = plt.subplots(4, 1, sharex=True)\n",
    "    xvals = np.arange(data.shape[0])\n",
    "    \n",
    "    ax[0].plot(xvals, data[:, 0])\n",
    "    ax[1].plot(xvals, target[:, 0])\n",
    "    ax[2].plot(xvals, target[:, 1])\n",
    "    ax[3].plot(xvals, target[:, 2])\n",
    "\n",
    "    # Let's end it here\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f98f4e-c5b4-4d6a-80c6-de3ed0eff9a9",
   "metadata": {},
   "source": [
    "Last thing: Do the same for the test dataset too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854d3c6-53d0-407e-9f3d-590c7e830865",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_test = 'events_phases_Zonly_TEST.h5'\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    Hdf5DataGenerator(),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 6000, 1), dtype=tf.float32, name='data'),\n",
    "        tf.TensorSpec(shape=(None, 6000, 3), dtype=tf.float32, name='targets')\n",
    "    ), \n",
    "    args=(filename_test, batch_size)\n",
    ")\n",
    "\n",
    "test_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3786be-e998-4eb8-9093-1d7f078dd3d6",
   "metadata": {},
   "source": [
    "## Construct a model\n",
    "\n",
    "With data and labels in place, we are ready to construct the first model. We'll give you this nearly for free, but not entirely:\n",
    "\n",
    "### Exercise: \n",
    "\n",
    "Identify how many outputs the model should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db9fb8-9832-45ee-bb3c-8e6e84956193",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outputs = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d90c6a-0572-46ee-8047-6b3489eed608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model(num_outputs):\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(shape=(6000, 1)),\n",
    "        \n",
    "        # Downsample\n",
    "        keras.layers.Conv1D(16, 3, padding='same', activation='relu'),\n",
    "        keras.layers.MaxPooling1D(2),\n",
    "        keras.layers.Conv1D(32, 3, padding='same', activation='relu'),\n",
    "        keras.layers.MaxPooling1D(2),\n",
    "        keras.layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "        keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        # Process\n",
    "        keras.layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "        keras.layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "        \n",
    "        # Upsample\n",
    "        keras.layers.Conv1DTranspose(32, 3, strides=2, padding='same', activation='relu'),\n",
    "        keras.layers.Conv1DTranspose(16, 3, strides=2, padding='same', activation='relu'),\n",
    "        keras.layers.Conv1DTranspose(8, 3, strides=2, padding='same', activation='relu'),\n",
    "        \n",
    "        # Output\n",
    "        keras.layers.Conv1D(num_outputs, 1, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab746c90-0774-415a-b5fd-dcd209e405b4",
   "metadata": {},
   "source": [
    "Compile and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82877c73-bd03-4534-95bd-9458705813ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = create_sequential_model(number_of_outputs)\n",
    "\n",
    "sequential_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4), loss=\"categorical_crossentropy\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "sequential_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c56f3-65cc-4ab1-b26b-a61a96bb3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c7681-ae8b-42e8-b128-376db9d72b0f",
   "metadata": {},
   "source": [
    "## Examine the model\n",
    "\n",
    "Let's see visually how well the model can pick incoming phases. Since we want to look at different models, it's convenient to write a function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d430b1d-27b7-45fd-9652-ab6a435acaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_event(model):\n",
    "    \n",
    "    for batch_data, batch_target in train_dataset.take(1):\n",
    "        \n",
    "        data = batch_data[1]\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "        target = batch_target[1]\n",
    "        target = np.expand_dims(target, axis=0)\n",
    "    \n",
    "        preds = model.predict(data)\n",
    "    \n",
    "        _, ax = plt.subplots(4, 1, sharex=True, figsize=(12,8))\n",
    "        xvals = np.arange(data.shape[1])\n",
    "    \n",
    "        ax[0].plot(xvals, data[0, :, 0])\n",
    "        ax[0].set_ylabel('Data')\n",
    "        \n",
    "        ax[1].plot(xvals, preds[0, :, 0], color='blue')\n",
    "        ax[1].set_ylabel('P similarity')\n",
    "        ax[1].set_ylim(0, 1)\n",
    "        \n",
    "        ax[2].plot(xvals, preds[0, :, 1], color='red')\n",
    "        ax[2].set_ylabel('S similarity')\n",
    "        ax[2].set_ylim(0, 1)\n",
    "        \n",
    "        ax[3].plot(xvals, preds[0, :, 2], color='green')\n",
    "        ax[3].set_ylabel('Noise')\n",
    "        ax[3].set_ylim(0, 1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229b71e-0e9c-4f7e-a274-411a1dbe1c8f",
   "metadata": {},
   "source": [
    "And call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275dab3-bb25-42f5-a49b-b5f0f79a528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_event(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7d08f-de94-4118-afcc-2290300abaa2",
   "metadata": {},
   "source": [
    "For the \"real\" test, we want to use all the events in the testing dataset, and compute the average performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29964f5e-264c-4b40-89b1-b5b3944a3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ff149-846b-4051-adde-e4efdf66f8a6",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "See if adding more layers will considerably improve this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db90247-1250-486e-80ce-b29faca7544c",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Is \"accuracy\" the correct metric to use for evaluating performance in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4eb894-b265-4f94-b026-db9520f00026",
   "metadata": {},
   "source": [
    "## Construct a more complicated model \n",
    "\n",
    "It's time to go beyond the limits of the sequential model, and create something with more complicated flow, like this:\n",
    "\n",
    "![](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/gji/239/2/10.1093_gji_ggae298/1/ggae298fig2.jpeg?Expires=1761155332&Signature=ELkc-i-qYmbYZuw8HU9hdyjj2VIchEEOQpjNLJVpSl8ZS2yhyBxsgIwa5hl2fWWwb6rA2cdk6I9T-DJyAVmoiyzgFacZvtZymKYY-y0D4xdymmD7BT5Z-eZQgcmxPZMdiaxhRPFbqSTbGpv-KeY1bR5a5VYobth-T0nXmKqwtt94LrzZsniGoWKJlsIkGVXW5Z6zW0OmVrsTIHTKUxlnFsPz9fRqw~A4YwdQe0arxYrltPzu5waMvynDfYfv4to6i2496ltey1~c~6QqpIU7U7wNWKNK6jlVpZU5t3XRXYYaw6JAw8XeD~cWQvuN8riTf1GEH5RGL6Ymz7TsuFKQJw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)\n",
    "\n",
    "This structure requires the use of Keras' [_functional API_](https://keras.io/guides/functional_api/). It's somewhat complicated to get into, so we can start from a minimal example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b3bab-b44b-49ea-9a00-efbff62c89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_small_phasenet(num_outputs):\n",
    "\n",
    "    inputs = keras.layers.Input(shape=(6000, 1))\n",
    "\n",
    "    # Encoder\n",
    "    # Level 1\n",
    "    c1 = keras.layers.Conv1D(16, 9, padding='same', activation='relu')(inputs)\n",
    "    c1 = keras.layers.Conv1D(16, 9, padding='same', activation='relu')(c1)\n",
    "    p1 = keras.layers.MaxPooling1D(2)(c1)\n",
    "\n",
    "    # Level 2\n",
    "    c2 = keras.layers.Conv1D(32, 7, padding='same', activation='relu')(p1)\n",
    "    c2 = keras.layers.Conv1D(32, 7, padding='same', activation='relu')(c2)\n",
    "    p2 = keras.layers.MaxPooling1D(2)(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = keras.layers.Conv1D(64, 5, padding='same', activation='relu')(p2)\n",
    "    c3 = keras.layers.Conv1D(64, 5, padding='same', activation='relu')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    # Level 2\n",
    "    u2 = keras.layers.UpSampling1D(2)(c3)\n",
    "    u2 = keras.layers.Concatenate()([u2, c2])\n",
    "    c4 = keras.layers.Conv1D(32, 7, padding='same', activation='relu')(u2)\n",
    "    c4 = keras.layers.Conv1D(32, 7, padding='same', activation='relu')(c4)\n",
    "\n",
    "    # Level 1\n",
    "    u1 = keras.layers.UpSampling1D(2)(c4)\n",
    "    u1 = keras.layers.Concatenate()([u1, c1])\n",
    "    c5 = keras.layers.Conv1D(16, 9, padding='same', activation='relu')(u1)\n",
    "    c5 = keras.layers.Conv1D(16, 9, padding='same', activation='relu')(c5)\n",
    "\n",
    "    # Output\n",
    "    outputs = keras.layers.Conv1D(num_outputs, 1, activation='softmax')(c5)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='small_phasenet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6026ba-20c9-47fa-911b-069d2509536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_phasenet = create_small_phasenet(number_of_outputs)\n",
    "\n",
    "# Optional: Plot the model as a graph\n",
    "keras.utils.plot_model(small_phasenet, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce31e1d-2301-4e7f-9fb2-db633940f8b3",
   "metadata": {},
   "source": [
    "Time to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcb387-1692-45ae-b921-7629e163d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55391b-ddfe-42a0-8275-541850249f98",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot an event, and evaluate results for the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1b6cb-f510-46b8-8d3a-08f5c403c270",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "With bigger models comes bigger risk of numerical instability during training. To mitigate this, we can introduce [`BatchNormalization`](https://keras.io/api/layers/normalization_layers/batch_normalization/) layers. Usually we would put these between a convolutional layer and the activation function, like so:\n",
    "\n",
    "```\n",
    "c1 = keras.layers.Conv1D(16, 9, padding='same')(inputs)\n",
    "c1 = keras.layers.BatchNormalization()(c1)\n",
    "c1 = keras.layers.Activation('relu')(c1)\n",
    "```\n",
    "\n",
    "Add such layers to the model, and see if things improve, especially if you add additional Conv1D blocks as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b666f0d-6d4f-4f2e-95f5-b313e0da5776",
   "metadata": {},
   "source": [
    "## Construct an _even_ more complicated model\n",
    "\n",
    "To show how the _functional API_ can make it somewhat easier to build complex models, have a look at the code below. This is also a PhaseNet-type model, but where we add layers in _loops_, instead of writing each layer explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ccfc5-a6ad-44cf-9eae-8dd2bb072777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigger_phasenet():\n",
    "    \n",
    "    inputs = keras.Input(shape=(6000, 1))\n",
    "\n",
    "    # First half of the network: downsampling inputs\n",
    "\n",
    "    # Entry block\n",
    "    x = keras.layers.Conv1D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside for residual (aka. skip) connection\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 64, 64]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.MaxPooling1D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual, and add it\n",
    "        residual = keras.layers.Conv1D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = keras.layers.add([x, residual])\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Second half: upsampling inputs\n",
    "\n",
    "    for filters in [64, 64, 64, 32]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv1DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv1DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.UpSampling1D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = keras.layers.UpSampling1D(2)(previous_block_activation)\n",
    "        residual = keras.layers.Conv1D(filters, 1, padding=\"same\")(residual)\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Get the correct output shape\n",
    "    outputs = keras.layers.Conv1D(3, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad3e04-afe4-4387-be61-ef756129d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_phasenet = create_bigger_phasenet()\n",
    "big_phasenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ce577-1717-4c35-88b5-5d5514e5178b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Train the bigger model! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
